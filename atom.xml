<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Welcome to Junlei&#39;s word!</title>
  
  
  <link href="http://junlei-zhou.com/atom.xml" rel="self"/>
  
  <link href="http://junlei-zhou.com/"/>
  <updated>2024-06-20T06:04:02.163Z</updated>
  <id>http://junlei-zhou.com/</id>
  
  <author>
    <name>坚竹韧周</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>symbols</title>
    <link href="http://junlei-zhou.com/2024/06/20/symbols/"/>
    <id>http://junlei-zhou.com/2024/06/20/symbols/</id>
    <published>2024-06-20T06:04:02.000Z</published>
    <updated>2024-06-20T06:04:02.163Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Mamba</title>
    <link href="http://junlei-zhou.com/2024/06/19/Mamba/"/>
    <id>http://junlei-zhou.com/2024/06/19/Mamba/</id>
    <published>2024-06-19T02:35:00.000Z</published>
    <updated>2024-06-20T09:53:43.748Z</updated>
    
    <content type="html"><![CDATA[<h1 id="mamba">Mamba</h1><!--本文是根据Mamba论文原作者撰写的Blog进行阅读的重点解读--><p>The Transformer architecture has been a major component in thesuccess of Large Language Models (LLMs). It has been used for nearly allLLMs that are being used today, from open-source models like Mistral toclosed-source models like ChatGPT.</p><p>To further improve LLMs, new architectures are developed that mighteven outperform the Transformer architecture. One of these methods is<em>Mamba</em>, a <em>State Space Model</em>.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6bc1ca-a387-47ed-a9a6-077af838b359_1148x892.png" style="zoom:50%;" /></p><p>Mamba was proposed in the paper <ahref="https://arxiv.org/abs/2312.00752">Mamba: Linear-Time SequenceModeling with Selective State Spaces</a>.<ahref="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-1-141228095">1</a>You can find its official implementation and model checkpoints in its <ahref="https://github.com/state-spaces/mamba">repository</a>.</p><p>In this post, I will introduce the field of State Space Models in thecontext of language modeling and explore concepts one by one to developan intuition about the field. Then, we will cover how Mamba mightchallenge the Transformers architecture.</p><p>As a visual guide, expect many visualizations to develop an intuitionabout Mamba and State Space Models!</p><h2 id="part-1-the-problem-with-transformers">Part 1: The Problem withTransformers</h2><p>To illustrate why Mamba is such an interesting architecture, let’s doa short re-cap of transformers first and explore one of itsdisadvantages.</p><p>A Transformer sees any textual input as a <em>sequence</em> thatconsists of <em>tokens</em>.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b8c299a-c0c0-46fe-86cf-b22e08a91b32_1776x544.png" style="zoom:50%;" /></p><p>A major benefit of Transformers is that whatever input it receives,it can look back at any of the earlier tokens in the sequence to deriveits representation.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c01c75-1105-4aeb-a608-f00c85bbe5f7_1776x532.png" style="zoom:50%;" /></p><h3 id="the-core-components-of-transformers">The Core Components ofTransformers</h3><p>Remember that a Transformer consists of two structures, a set ofencoder blocks for representing text and a set of decoder blocks forgenerating text. Together, these structures can be used for severaltasks, including translation.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62a21d60-2e84-4c19-a6fb-d2eff501af1c_1776x952.png" style="zoom:50%;" /></p><p>We can adopt this structure to create generative models by using onlydecoders. This Transformer-based model, <em>Generative Pre-trainedTransformers</em> (GPT), uses decoder blocks to complete some inputtext.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58e51959-d4ef-4fa9-a1c7-dab0e5ca4dc0_1776x1012.png" style="zoom: 50%;" /></p><p>Let’s take a look at how that works!</p><h3 id="a-blessing-with-training">A Blessing with Training…</h3><p>A single decoder block consists of two main components, maskedself-attention followed by a feed-forward neural network.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5af9c5-5266-4c2b-b583-b20a19f19fcc_1776x464.png" style="zoom:50%;" /></p><p>Self-attention is a major reason why these models work so well. Itenables an uncompressed view of the entire sequence with fasttraining.</p><p>So how does it work?</p><p>It creates a matrix comparing each token with every token that camebefore. The weights in the matrix are determined by how relevant thetoken pairs are to one another.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F167cfe80-2863-47c8-a969-cb2eeedbd353_1776x860.png" style="zoom:50%;" /></p><p>During training, this matrix is created in one go. The attentionbetween “<em>My</em>” and “<em>name</em>” does not need to be calculatedfirst before we calculate the attention between “<em>name</em>” and“<em>is</em>”.</p><p>It enables <strong>parallelization</strong>, which speeds up trainingtremendously!</p><h3 id="and-the-curse-with-inference">And the Curse with Inference!</h3><p>There is a flaw, however. When generating the next token, we need tore-calculate the attention for the <em>entire sequence</em>, even if wealready generated some tokens.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb66f1965-fc44-4a61-b9c6-912c8120ecad_2420x580.png" style="zoom:50%;" /></p><p>Generating tokens for a sequence of length <em>L</em> needs roughly<em>L²</em> computations which can be costly if the sequence lengthincreases.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F405074ed-aa8c-44b4-88a5-bae1dad0412e_2072x392.png" style="zoom:50%;" /></p><p>This need to recalculate the entire sequence is a major bottleneck ofthe Transformer architecture.</p><p>Let’s look at how a “classic” technique, Recurrent Neural Networks,solves this problem of slow inference.</p><hr /><blockquote><p><strong>为什么Transformer的推理速度会很慢？</strong></p><p>因为Transformer在推理当前的状态时，需要计算包含之前所有状态的信息，也就是说，当计算到第N个状态时，需要同时计算包含前N个状态的信息以得出当前状态的取值，这也就时为什么以Transformer为基础的模型在输入时会有长度限制的前提条件。而相比较于Transformer，RNN模型有着比较快的推理速度因为它只用重点关注前面重点的状态，根据此来推理出当前状态，故而其计算速度快，但由于只关注之前的部分状态，也导致了其推理的准确度相比较于Transformer略有不及。</p><p>例如：当输入的Batch Size=b, Sequence length=N,那么一个具有 <spanclass="math inline">\(l\)</span> 层的Transformer模型的计算量为 <spanclass="math inline">\(l*(24bNd^2+4bN^2d)\)</span>, <spanclass="math inline">\(d\)</span>为词向量的维度或者是隐藏层的维度，详细的计算过程见<ahref="https://blog.csdn.net/v_JULY_v/article/details/133619540">原文</a>。</p></blockquote><h3 id="are-rnns-a-solution">Are RNNs a Solution?</h3><p>Recurrent Neural Networks (RNN) is a sequence-based network. It takestwo inputs at each time step in a sequence, namely the input at timestep <strong><em>t</em></strong> and a hidden state of the previous timestep <strong><em>t-1</em></strong>, to generate the next hidden stateand predict the output.</p><p>RNNs have a looping mechanism that allows them to pass informationfrom a previous step to the next. We can “unfold” this visualization tomake it more explicit.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddc71706-8da8-4c28-921b-675e9164c7ab_2404x872.png" style="zoom:50%;" /></p><p>When generating the output, the RNN only needs to consider theprevious hidden state and current input. It prevents recalculating allprevious hidden states which is what a Transformer would do.</p><p>In other words, RNNs can do inference fast as it scales linearly withthe sequence length! In theory, it can even have an <em>infinite contextlength</em>.</p><p>To illustrate, let’s apply the RNN to the input text we have usedbefore.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2484541-f7b1-4950-b04f-5a3177596fbb_2228x808.png" style="zoom:50%;" /></p><p>Each hidden state is the aggregation of all previous hidden statesand is typically a compressed view.</p><p>There is a problem, however…</p><p>Notice that the last hidden state, when producing the name“<em>Maarten</em>” does not contain information about the word“<em>Hello</em>” anymore. RNNs tend to forget information over timesince they only consider one previous state.</p><p>This sequential nature of RNNs creates another problem. Trainingcannot be done in parallel since it needs to go through each step at atime sequentially.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819fcf9b-ea31-4954-8496-4b66c5b46dc2_2236x828.png" style="zoom:50%;" /></p><p>The problem with RNNs, compared to Transformers, is completely theopposite! Its inference is incredibly fast but it is notparallelizable.</p><blockquote><p><strong>RNN存在的问题</strong></p><ol type="1"><li><p>虽然在RNN中，每个状态都是先前所有隐藏状态的聚合，然而随着时间的推移，RNN会忘记掉一部分信息。</p></li><li><p>RNN没办法并行训练，即推理速度快但训练慢，而且因为RNN的结构，也无法进行卷积运算。</p></li></ol><p>由于RNN的每个时间 <span class="math inline">\(t\)</span>的输出需要依赖前一个时间 <span class="math inline">\(t-1\)</span> 的输出，导致其循环操作无法并行训练。</p><p>RNN中通过循环结构来实现权重共享，而CNN中通过卷积操作实现局部链接和全局共享。</p></blockquote><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7af6befe-e99d-4350-b555-21ce543cae53_2072x580.png" style="zoom:50%;" /></p><p>Can we somehow find an architecture that does parallelize traininglike Transformers whilst still performing inference that scales linearlywith sequence length?</p><p>Yes! This is what Mamba offers but before diving into itsarchitecture, let’s explore the world of State Space Models first.</p><h2 id="part-2-the-state-space-model-ssm">Part 2: The <strong>StateSpace Model (SSM)</strong></h2><p>A State Space Model (SSM), like the Transformer and RNN, processessequences of information, like text but also signals. In this section,we will go through the basics of SSMs and how they relate to textualdata.</p><h3 id="what-is-a-state-space">What is a State Space?</h3><p>A State Space contains the minimum number of variables that fullydescribe a system. It is a way to mathematically represent a problem bydefining a system's possible states.</p><p>Let’s simplify this a bit. Imagine we are navigating through a maze.The “<em>state space</em>” is the map of all possible locations(states). Each point represents a unique position in the maze withspecific details, like how far you are from the exit.</p><p>The “<em>state space representation</em>” is a simplified descriptionof this map. It shows where you are (current state), where you can gonext (possible future states), and what changes take you to the nextstate (going right or left).</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6480800-2449-456a-87a7-27c8a4e9e718_2520x1388.png" style="zoom:50%;" /></p><p>Although State Space Models use equations and matrices to track thisbehavior, it is simply a way to track where you are, where you can go,and how you can get there.</p><p>The variables that describe a state, in our example the X and Ycoordinates, as well as the distance to the exit, can be represented as“<em>state vectors</em>”.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c79eba-2559-4d9d-8999-bee33666f2e3_2364x736.png" style="zoom:50%;" /></p><p>Sounds familiar? That is because embeddings or vectors in languagemodels are also frequently used to describe the “state” of an inputsequence. For instance, a vector of your current position (state vector)could look a bit like this:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ff8812a-64d2-4fc6-8e54-eb86222333b0_1496x444.png" style="zoom:50%;" /></p><p>In terms of neural networks, the “state” of a system is typically itshidden state and in the context of Large Language Models, one of themost important aspects of generating a new token.</p><p>What is a State Space Model?</p><p>SSMs are models used to describe these state representations and makepredictions of what their next state could be depending on someinput.</p><p>Traditionally, at time <strong><em>t</em></strong>, SSMs:</p><ul><li><p>map an input sequence <strong><em>x(t)</em></strong> — (e.g.,moved left and down in the maze)</p></li><li><p>to a latent state representation <strong><em>h(t)</em></strong> —(e.g., distance to exit and x/y coordinates)</p></li><li><p>and derive a predicted output sequence<strong><em>y(t)</em></strong> — (e.g., move left again to reach theexit sooner)</p><blockquote><p><strong>A:</strong> 当前状态如何影响下一状态</p><p><strong>B:</strong> 输入序列 <strong>x</strong>如何影响当前的状态</p><p><strong>C:</strong> 当前的状态如何影响当前的输出</p><p><strong>D:</strong> 输入序列 <strong>x</strong>如何影响当前的输出</p></blockquote></li></ul><p>However, instead of using <em>discrete</em> <em>sequences</em> (likemoving left once) it takes as input a <em>continuous</em><em>sequence</em> and predicts the output sequence.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5788c3e4-8794-4492-af87-3a45f7a6aa70_1992x624.png" style="zoom:50%;" /></p><p>SSMs assume that dynamic systems, such as an object moving in 3Dspace, can be predicted from its state at time<strong><em>t</em></strong> through two equations.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F32401c6d-39b6-4619-a75e-6b33d3268bca_2520x388.png" style="zoom:50%;" /></p><blockquote><p>这里的 <span class="math inline">\(h&#39;(t)\)</span>是SSM一阶微分方程中的导数，指的是连续状态下的当前状态是由上一状态得出的。</p><p>写成离散形态的即为 <spanclass="math inline">\(h_t=Ah_{t-1}+Bx_t\)</span>.</p><p>通过求解这两个方程，可以根据观察到的数据“输入序列 x 和先前的状态 h(t)，即可实现对未来状态 y(t) 进行预测。”</p></blockquote><p>By solving these equations, we assume that we can uncover thestatistical principles to predict the state of a system based onobserved data (input sequence and previous state).</p><p>==Its goal is to find this state representation<strong><em>h(t)</em></strong> such that we can go from an input to anoutput sequence.==</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca5c7ae-3dbe-44d8-8b13-7f4dcc14a29b_2008x624.png" style="zoom:50%;" /></p><p>These two equations are the core of the State Space Model.</p><p>The two equations will be referenced throughout this guide. To makethem a bit more intuitive, they are <strong>color-coded</strong> so youcan quickly reference them.</p><p>The <strong>state equation</strong> describes how the state changes(through <em>matrix A</em>) based on how the input influences the state(through <em>matrix B</em>).</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0876819d-8a46-4187-9826-14391bfd47b9_1796x624.png" style="zoom:50%;" /></p><p>As we saw before, <strong><em>h(t)</em></strong> refers to our latentstate representation at any given time <strong><em>t</em></strong>, and<strong><em>x(t)</em></strong> refers to some input.</p><p>The <strong>output equation</strong> describes how the state istranslated to the output (through <em>matrix C</em>) and how the inputinfluences the output (through <em>matrix D</em>).</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2e87708-9676-4a1c-b32c-d703026f64d9_1796x624.png" style="zoom:50%;" /></p><blockquote><p><strong>NOTE</strong>: Matrices <em>A</em>, <em>B</em>, <em>C</em>,and <em>D</em> are also commonly refered to as <em>parameters</em> sincethey are learnable.</p></blockquote><p>Visualizing these two equations gives us the followingarchitecture:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc358439e-c507-49f1-ac2e-5dedaccc2a8b_1728x364.png" style="zoom:50%;" /></p><p>Let’s go through the general technique step-by-step to understand howthese matrices influence the learning process.</p><p>Assume we have some input signal <strong><em>x(t)</em></strong>, thissignal first gets multiplied by <em>matrix B</em> which describes howthe inputs influence the system.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd6f8dae-2281-47af-8ba3-06bbdc594d1c_1956x360.png" style="zoom: 67%;" /></p><p>The updated state (akin to the hidden state of a neural network) is alatent space that contains the core “knowledge” of the environment. Wemultiply the state with <em>matrix A</em> which describes how all theinternal states are connected as they represent the underlying dynamicsof the system.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cedc98a-d200-4fe4-b311-6d68dcaa50af_1956x572.png" style="zoom:50%;" /></p><blockquote><p><strong>A</strong>矩阵决定了整个状态的演化，即如何从一个状态演化到下一个状态。其本质为存储着之前所有历史信息的浓缩精华。</p></blockquote><p>As you might have noticed, <em>matrix A</em> is applied beforecreating the state representations and is updated after the staterepresentation has been updated.</p><p>Then, we use <em>matrix C</em> to describe how the state can betranslated to an output.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8599487f-1023-4069-be7a-8056e63b0574_1956x572.png" style="zoom:50%;" /></p><p>Finally, we can make use of <em>matrix D</em> to provide a directsignal from the input to the output. This is also often referred to as a<em>skip-connection</em>.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf79721f-5cef-44da-98c5-f63a7839ebc3_1956x756.png" style="zoom:50%;" /></p><p>Since <em>matrix D</em> is similar to a skip-connection, the SSM isoften regarded as the following without the skip-connection.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca1d511-7d31-42a0-9220-2fb85b256efd_1956x864.png" style="zoom:50%;" /></p><p>Going back to our simplified perspective, we can now focus onmatrices <em>A</em>, <em>B</em>, and <em>C</em> as the core of theSSM.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e52f4f0-d7ad-453d-a741-6dfa4a998964_1728x352.png" style="zoom:50%;" /></p><p>We can update the original equations (and add some pretty colors) tosignify the purpose of each matrix as we did before.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55df8ede-3a16-4473-8ea9-872fe199d3a1_1904x676.png" style="zoom:50%;" /></p><p>Together, these two equations aim to predict the state of a systemfrom observed data. Since the input is expected to be continuous, themain representation of the SSM is a <strong>continuous-timerepresentation</strong>.</p><h3 id="from-ssm-to-s4">From SSM to S4</h3><h4 id="from-a-continuous-to-a-discrete-signal">From a Continuous to aDiscrete Signal</h4><p>Finding the state representation <strong><em>h(t)</em></strong> isanalytically challenging if you have a continuous signal. Moreover,since we generally have a discrete input (like a textual sequence), wewant to discretize the model.</p><p>To do so, we make use of the <em>Zero-order hold technique.</em> Itworks as follows.</p><ol type="1"><li>First, every time we receive a discrete signal, we hold its valueuntil we receive a new discrete signal. This process creates acontinuous signal the SSM can use:</li></ol><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a2ffb18-2e66-4135-9888-98e9ab88d0d8_1488x472.png" style="zoom:50%;" /></p><ol start="2" type="1"><li><p>How long we hold the value is represented by a new learnableparameter, called the <em>step size</em> <spanclass="math inline">\(\Delta\)</span> . It represents the resolution ofthe input.</p></li><li><p>Now that we have a continuous signal for our input, we cangenerate a continuous output and only sample the values according to thetime steps of the input.</p></li></ol><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F042ff699-81af-4479-b99f-92e4997c4c81_1488x476.png" style="zoom:50%;" /></p><p>These sampled values are our discretized output!</p><p>Mathematically, we can apply the <ahref="https://en.wikipedia.org/wiki/Zero-order_hold">Zero-order hold</a>as follows:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6df4b59-6f76-4f13-a201-7b69e59df164_6200x1176.png" style="zoom:50%;" /></p><p>Together, they allow us to go from a continuous SSM to a discrete SSMrepresented by a formulation that instead of a<em>function-to-function</em>, <spanclass="math inline">\(x(t)\rightarrow y(t)\)</span>, is now a<em>sequence-to-sequence</em>, <spanclass="math inline">\(x_k\rightarrow y_k\)</span>:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc29cfbbb-ae41-4dc2-b899-9e0a81cba34d_1980x1012.png" style="zoom:50%;" /></p><p>Here, matrices <strong><em>A</em></strong> and<strong><em>B</em></strong> now represent discretized parameters of themodel.</p><p>We use <strong><em>k</em></strong> instead of<strong><em>t</em></strong> to represent discretized timesteps and tomake it a bit more clear when we refer to a continuous versus a discreteSSM.</p><blockquote><p><strong>NOTE:</strong> We are still saving the continuous form of<em>Matrix A</em> and not the discretized version during training.During training, the continuous representation is discretized.</p></blockquote><p>Now that we have a formulation of a discrete representation, let’sexplore how we can actually <em>compute</em> the model.</p><h4 id="the-recurrent-representation">The Recurrent Representation</h4><p>Our discretized SSM allows us to formulate the problem in specifictimesteps instead of continuous signals. A recurrent approach, as we sawbefore with RNNs is quite useful here.</p><p>If we consider discrete timesteps instead of a continuous signal, wecan reformulate the problem with timesteps:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b70ba4-b068-44d1-8641-9b224d103c51_1980x548.png" style="zoom:50%;" /></p><p>At each timestep, we calculate how the current input (<strong><spanclass="math inline">\(Bx_k\)</span></strong>) influences the previousstate (<strong><span class="math inline">\(Ah_{k-1}\)</span></strong>)and then calculate the predicted output (<strong><spanclass="math inline">\(Ch_k\)</span></strong>).</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb4d0412-87fb-4507-bedb-4793588bd465_2116x788.png" style="zoom:50%;" /></p><p>This representation might already seem a bit familiar! We canapproach it the same way we did with the RNN as we saw before.</p><blockquote><p>对于 <span class="math inline">\(y_2\)</span> 展开计算则有 <spanclass="math display">\[\begin{align}y_2&amp;=Ch_2 \\&amp;=C(\bar{A}h_1+\bar{B}x_2) \\&amp;=C(\bar{A}(\bar{A}h_0+Bx_1)+\bar{B}x_2) \\&amp;=C(\bar{A}(\bar{A} \cdot \bar{B}x_0 +Bx_1)+\bar{B}x_2) \\&amp;=C\cdot \bar{A}^2\cdot \bar{B}x_0+C\cdot \bar{A}\cdot\bar{B}x_1+C\cdot \bar{B}x_2\end{align}\]</span> 由此可推得 <span class="math inline">\(y_k=C\cdot \bar{A}^k\cdot \bar{B}x_0+C\cdot \bar{A}^{k-1} \cdot \bar{B}x_1+\cdots+C\cdot\bar{A} \cdot \bar{B}x_k\)</span></p></blockquote><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91ca51f7-9b9b-4f17-bccb-32a5a96f3339_2184x868.png" style="zoom:50%;" /></p><p>Which we can unfold (or unroll) as such:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1084e8a-a70d-450b-beb0-f18117ade5ed_2184x876.png" style="zoom:50%;" /></p><p>Notice how we can use this discretized version using the underlyingmethodology of an RNN.</p><p>This technique gives us both the advantages and disadvantages of anRNN, namely fast inference and slow training.</p><h4 id="the-convolution-representation">The ConvolutionRepresentation</h4><p>Another representation that we can use for SSMs is that ofconvolutions. Remember from classic image recognition tasks where weapplied filters (<em>kernels</em>) to derive aggregate features:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47f05950-bfad-4013-b854-679c9a47ada9_3216x2144.png" style="zoom:50%;" /></p><p>Since we are dealing with text and not images, we need a1-dimensional perspective instead:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb943872f-de72-43e8-b2f1-cb8213f120a3_3216x1296.png" style="zoom:50%;" /></p><p>The kernel that we use to represent this “filter” is derived from theSSM formulation:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05049821-2352-4c04-8fb2-07fe15c20a9c_2620x824.png" style="zoom:50%;" /></p><blockquote><p><strong>为什么可以写成卷积的形式？</strong></p><p>如上图，其中 <span class="math inline">\(K\)</span>为卷积核，其中此时的 <strong>A、B、C</strong> 都是常数，可以看出卷积核<span class="math inline">\(K\)</span>是通过固定数值矩阵得到的，只需确定 <strong>A、B、C</strong>即可并行运算。</p></blockquote><p>Let’s explore how this kernel works in practice. Like convolution, wecan use our SSM kernel to go over each set of tokens and calculate theoutput:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9007d03b-c1c9-4b37-8c83-f27bfe8318f4_2620x1080.png" style="zoom:50%;" /></p><p>This also illustrates the effect padding might have on the output. Ichanged the order of padding to improve the visualization but we oftenapply it at the end of a sentence.</p><p>In the next step, the kernel is moved once over to perform the nextstep in the calculation:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ed71fb-f237-4173-bb23-bd1bf02ff123_2620x1080.png" style="zoom:50%;" /></p><p>In the final step, we can see the full effect of the kernel:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4387b79-5e92-4fe2-8b30-2abf112f5a73_2620x1080.png" style="zoom:50%;" /></p><p>A major benefit of representing the SSM as a convolution is that itcan be trained in parallel like Convolutional Neural Networks (CNNs).However, due to the fixed kernel size, their inference is not as fastand unbounded as RNNs.</p><h4 id="the-three-representations">The Three Representations</h4><p>These three representations, <em>continuous</em>, <em>recurrent</em>,and <em>convolutional</em> all have different sets of advantages anddisadvantages:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F682187d6-f402-44aa-8097-8a2e5b6179a7_2072x744.png" style="zoom:50%;" /></p><p>Interestingly, we now have efficient inference with the recurrent SSMand parallelizable training with the convolutional SSM.</p><blockquote><ol type="1"><li>在训练的时候怎么使用CNN进行训练？</li></ol><p><span class="math display">\[y=\bar{K}x\]</span></p><p>​ 能够通过使用卷积的方法使其进行并行计算。</p><ol start="2" type="1"><li>在推理的时候怎么进行推理？ <span class="math display">\[\left\{ \begin{array}{lcl}h_k=\bar{A}h_{k-1}+\bar{B}x_k \\y_k=Ch_k+Dx_k\end{array}\right.\]</span></li></ol></blockquote><p>With these representations, there is a neat trick that we can use,namely choose a representation depending on the task. During training,we use the convolutional representation which can be parallelized andduring inference, we use the efficient recurrent representation:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c43c82d-9735-4d55-97bb-8ad6f504909e_1960x1008.png" style="zoom:50%;" /></p><p>This model is referred to as the <ahref="https://proceedings.neurips.cc/paper_files/paper/2021/hash/05546b0e38ab9175cd905eebcc6ebb76-Abstract.html">LinearState-Space Layer (LSSL)</a>.<ahref="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-2-141228095">2</a></p><p>These representations share an important property, namely that of<strong><em>Linear Time Invariance</em></strong> (LTI). LTI states thatthe SSMs parameters, <em>A</em>, <em>B</em>, and <em>C</em>, are fixedfor all timesteps. This means that matrices <em>A</em>, <em>B</em>, and<em>C</em> are the same for every token the SSM generates.</p><p>In other words, regardless of what sequence you give the SSM, thevalues of <em>A</em>, <em>B</em>, and <em>C</em> remain the same. Wehave a static representation that is not content-aware.</p><p>Before we explore how Mamba addresses this issue, let’s explore thefinal piece of the puzzle, <em>matrix A</em>.</p><h4 id="the-importance-of-matrix-a">The Importance of Matrix<em>A</em></h4><blockquote><p><strong>A</strong>矩阵决定了整个状态的演化，即如何从一个状态演化到下一个状态。其本质为存储着之前所有历史信息的浓缩精华，<strong>A</strong> 决定了系统的动态特性，但 <strong>A</strong>可能存在一个问题，即 <strong>A</strong>只记住了之前的部分状态，像RNN那样忘记了部分状态，从而会导致SSM在性能上与RNN相似。</p><p><strong>A</strong> 设计的关键是如何在有限的空间中保留更多的记忆！</p></blockquote><p>Arguably one of the most important aspects of the SSM formulation is<em>matrix A</em>. As we saw before with the recurrent representation,it captures information about the <em>previous</em> state to build the<em>new</em> state.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07542fb1-d4b6-421e-8b2a-a3f0a7790939_2028x876.png" style="zoom:50%;" /></p><p>In essence, <em>matrix</em> <em>A</em> produces the hidden state:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47635355-6b9a-4981-af3a-7ee6a12b87b3_1412x468.png" style="zoom:50%;" /></p><p>Creating <em>matrix A</em> can therefore be the difference betweenremembering only a few previous tokens and capturing every token we haveseen thus far. Especially in the context of the Recurrent representationsince it only <em>looks back</em> <em>at the previous state</em>.</p><p>So how can we create <em>matrix A</em> in a way that retains a largememory (context size)?</p><p>We use Hungry Hungry Hippo! Or <ahref="https://proceedings.neurips.cc/paper/2020/hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html">HiPPO</a><ahref="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-3-141228095">3</a>for <strong>Hi</strong>gh-order <strong>P</strong>olynomial<strong>P</strong>rojection <strong>O</strong>perators. HiPPO attemptsto compress all input signals it has seen thus far into a vector ofcoefficients.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07985a64-fc26-4ee8-9ec2-c488e4cb709a_1492x488.png" style="zoom:50%;" /></p><p>It uses <em>matrix A</em> to build a state representation thatcaptures recent tokens well and decays older tokens. Its formula can berepresented as follows:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bc7c768-7f0c-4983-a21e-70a4f587e6aa_2520x628.png" style="zoom:50%;" /></p><p>Assuming we have a square <em>matrix <strong>A</strong></em>, thisgives us:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8f5de9-6448-43d5-9878-c8cd1d938b7c_1436x708.png" style="zoom:50%;" /></p><p>Building <em>matrix A</em> using HiPPO was shown to be much betterthan initializing it as a random matrix. As a result, it more accuratelyreconstructs <em>newer</em> signals (recent tokens) compared to<em>older</em> signals (initial tokens).</p><p>The idea behind the HiPPO Matrix is that it produces a hidden statethat memorizes its history.</p><p>Mathematically, it does so by tracking the coefficients of a <ahref="https://proceedings.neurips.cc/paper/2019/hash/952285b9b7e7a1be5aa7849f32ffff05-Abstract.html">Legendrepolynomial</a> which allows it to approximate all of the previoushistory.<ahref="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-4-141228095">4</a></p><p>HiPPO was then applied to the recurrent and convolutionrepresentations that we saw before to handle long-range dependencies.The result was <a href="https://arxiv.org/abs/2111.00396">StructuredState Space for Sequences (S4)</a>, a class of SSMs that can efficientlyhandle long sequences.<ahref="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-5-141228095">5</a></p><p>It consists of three parts:</p><ul><li><p>State Space Models</p></li><li><p>HiPPO for handling <strong>long-rangedependencies</strong></p></li><li><p>Discretization for creating <strong>recurrent</strong> and<strong>convolution</strong> representations</p></li></ul><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb055ec5-f8c7-4862-ab88-f4fb38abf042_1892x844.png" style="zoom:50%;" /></p><p>This class of SSMs has several benefits depending on therepresentation you choose (recurrent vs. convolution). It can alsohandle long sequences of text and store memory efficiently by buildingupon the HiPPO matrix.</p><blockquote><p><strong>NOTE</strong>: If you want to dive into more of the technicaldetails on how to calculate the HiPPO matrix and build a S4 modelyourself, I would HIGHLY advise going through the <ahref="https://srush.github.io/annotated-s4/">Annotated S4</a>.</p><p>其中在S4中对 <strong>A</strong> 进行了改进：</p><p><strong>Theorem 1.</strong> All <em>HiPPO</em> matrices have a<em>Normal Plus Low-Rank</em> (NPLR) representation <spanclass="math display">\[A=VAV^*-PQ^\top=V(\Lambda-(V^*P)(V^*Q))V^*\]</span> for unitary <span class="math inline">\(V\in\mathbb{C}^{N\times N}\)</span>, diagonal <spanclass="math inline">\(\Lambda\)</span>， and low-rank factorization<span class="math inline">\(P,Q\in\mathbb{R}^{N\times r}\)</span>. Thesematrices <em>HiPPO-LegS, LegT, LagT</em> all satisfy <spanclass="math inline">\(r=1 \ or \ r=2\)</span>.</p></blockquote><hr /><h3 id="introduction-of-s4">Introduction of S4</h3><h4id="improving-transformer-struggles-with-handling-very-long-sequences">Improvingtransformer struggles with handling very long sequences</h4><p>序列数据一般都是离散的数据 比如文本、图、DNA</p><ol type="1"><li>但现实生活中还有很多连续的数据，比如音频、视频，对于音视频这种信号而言，其一个重要特点就是有极长的contextwindow</li><li>而在transformer长context上往往会失败，或者注意力机制在有着超长上下文长度的任务上并不擅长，而S4擅长这类任务。</li></ol><h4id="the-definition-and-derivation-of-hippostate-compresses-the-history-of-input">Thedefinition and derivation of HiPPO：State compresses the history ofinput</h4><p>我们已经知道一个RNN网络更擅长于处理这种序列数据，但是RNN网络最大的缺点是由于它的hiddenstate记忆能力有限，导致其会忘记掉一些以前的特征。</p><p>==关键：怎样改善RNN记忆有限的问题？==</p><p>假设我们在 <span class="math inline">\(t_0\)</span>时刻有接收到袁术输入信号 <spanclass="math inline">\(u(t)\)</span>之前的部分：</p><ol type="1"><li><p>我们希望使用一个记忆单元去压缩之前这段的输入部分来学习特征，我们使用一个多项式去近似这段之前的输入<span class="math display">\[x(t_0)=\begin{bmatrix} 0.1 \\-1.1 \\3.7 \\2.5\end{bmatrix}\]</span></p></li><li><p>当我们在接收更多的signal的时候，我们仍然希望这个记忆单元能够对已经接收到的所有的signal进行压缩，因此需要自动更新多项式的各项系数，如下图所示</p><p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_1.png" alt="Mamba HiPPO 1" border="0" style="zoom:50%;" ></p></li><li><p>此时，HiPPO的关键问题就变成了一个优化问题</p><ul><li><p>如何能够找到这些最优的近似？</p></li><li><p>如何快速的更新这些多项式的参数？</p></li></ul><p>因此，我们需要定义一个指标去判断一个近似的好坏程度。</p><p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_2.png" alt="Mamba HiPPO 2" border="0" style="zoom:50%;" ></p><p>(Note: 添加推导过程)</p></li><li><p>HiPPO的定义，两个矩阵 (<strong>A</strong>表示之前的状态怎样影响当前的状态，<strong>B</strong>当前的输入怎样影响当前的状态)与两个信号( <span class="math inline">\(x(t)\)</span> 当前状态之前接收到的signal与<span class="math inline">\(u(t)\)</span> 当前接收到的signal)</p><p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_3.png" alt="Mamba HiPPO 3" border="0" style="zoom:50%;" ></p><blockquote><p>需要澄清一点的是作者在这里使用了State spaceModel中状态的定义，与本文前面部分的定义有些许不同. 但事实上这里的 <spanclass="math inline">\(x(t)\)</span> 就等同于上文中的 <spanclass="math inline">\(h(t)\)</span> 这里的 <spanclass="math inline">\(u(t)\)</span> 就等同于上文中的 <spanclass="math inline">\(x(t)\)</span>。我们更换一下这个表示即可转变为上文中的形式 <spanclass="math display">\[x&#39;(t)=Ax(t)+Bu(t) \Longrightarrow h&#39;(t)=Ah(t)+Bx(t)\]</span> 其中 <strong>A</strong> 就是HiPPO matrix.</p></blockquote></li><li><p>HiPPO就相当于把一个高维的复杂函数映射压缩成一个简单的函数，这样既保留了基本信息，又节省了空间。如下图所示，<spanclass="math inline">\(u(t)\)</span> 是原始输入的signal，<spanclass="math inline">\(x(t)\)</span> 是经过压缩后产生到的signal，即对应上文中的 <span class="math inline">\(h(t)\)</span>.</p><p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_4.png" alt="Mamba HiPPO 4" border="0" style="zoom:50%;" ></p></li><li></li></ol><hr /><h2 id="part-3-mamba---a-selective-ssm">Part 3: Mamba - A SelectiveSSM</h2><p>We finally have covered all the fundamentals necessary to understandwhat makes Mamba special. State Space Models can be used to modeltextual sequences but still have a set of disadvantages we want toprevent.</p><p>In this section, we will go through Mamba’s two maincontributions:</p><ol type="1"><li><p>A <strong>selective scan algorithm</strong>, which allows themodel to filter (ir)relevant information</p></li><li><p>A <strong>hardware-aware algorithm</strong> that allows forefficient storage of (intermediate) results through <em>parallelscan</em>, <em>kernel fusion</em>, and <em>recomputation</em>.</p></li></ol><p>Together they create the <em>selective SSM</em> or <em>S6</em> modelswhich can be used, like self-attention, to create <em>Mambablocks</em>.</p><p>Before exploring the two main contributions, let’s first explore whythey are necessary.</p><h3 id="what-problem-does-it-attempt-to-solve">What Problem does itattempt to Solve?</h3><p>State Space Models, and even the S4 (Structured State Space Model),perform poorly on certain tasks that are vital in language modeling andgeneration, namely <em>the ability to focus on or ignore particularinputs</em>.</p><p>We can illustrate this with two synthetic tasks, namely<strong>selective copying</strong> and <strong>inductionheads</strong>.</p><p>In the <strong>selective copying</strong> task, the goal of the SSMis to copy parts of the input and output them in order:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3f1d1fb-7603-4f86-a2e4-88e0496a1f08_2120x464.png" style="zoom:50%;" /></p><p>However, a (recurrent/convolutional) SSM performs poorly in this tasksince it is <strong><em>Linear Time Invariant</em>.</strong> As we sawbefore, the matrices <em>A</em>, <em>B</em>, and <em>C</em> are the samefor every token the SSM generates.</p><p>As a result, an SSM cannot perform <em>content-aware reasoning</em>since it treats each token equally as a result of the fixed A, B, and Cmatrices. This is a problem as we want the SSM to reason about the input(prompt).</p><p>The second task an SSM performs poorly on is <strong>inductionheads</strong> where the goal is to reproduce patterns found in theinput:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27775742-18d2-4f2c-b792-5706976e75e3_2120x744.png" style="zoom:50%;" /></p><p>In the above example, we are essentially performing one-shotprompting where we attempt to “teach” the model to provide an“<strong><em>A:</em></strong>” response after every“<strong><em>Q:</em></strong>”. However, since SSMs are time-invariantit cannot select which previous tokens to recall from its history.</p><p>Let’s illustrate this by focusing on <em>matrix B</em>. Regardless ofwhat the input <strong><em>x</em></strong> is, <em>matrix B</em> remainsexactly the same and is therefore independent of<strong><em>x</em></strong>:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ee2bd7a-b99b-4871-8396-69cb7dd13cf5_1480x808.png" style="zoom:50%;" /></p><p>Likewise, <em>A</em> and <em>C</em> also remain fixed regardless ofthe input. This demonstrates the <em>static</em> nature of the SSMs wehave seen thus far.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fea51ca-8458-4216-bf1c-c880a23504b4_1412x484.png" style="zoom:50%;" /></p><p>In comparison, these tasks are relatively easy for Transformers sincethey <em>dynamically</em> change their attention based on the inputsequence. They can selectively “look” or “attend” at different parts ofthe sequence.</p><p>The poor performance of SSMs on these tasks illustrates theunderlying problem with time-invariant SSMs, the static nature ofmatrices <em>A</em>, <em>B</em>, and <em>C</em> results in problems with<em>content-awareness</em>.</p><h3 id="selectively-retain-information">Selectively RetainInformation</h3><p>The recurrent representation of an SSM creates a small state that isquite efficient as it compresses the entire history. However, comparedto a Transformer model which does no compression of the history (throughthe attention matrix), it is much less powerful.</p><p>Mamba aims to have the best of both worlds. A small state that is aspowerful as the state of a Transformer:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84b8a71a-6310-416b-8622-e9166593171e_1540x464.png" style="zoom:50%;" /></p><p>As teased above, it does so by compressing data selectively into thestate. When you have an input sentence, there is often information, likestop words, that does not have much meaning.</p><p>To selectively compress information, we need the parameters to bedependent on the input. To do so, let’s first explore the dimensions ofthe input and output in an SSM during training:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9376222e-b232-4458-a72c-bd741d8a031c_1436x500.png" style="zoom:50%;" /></p><p>In a Structured State Space Model (S4), the matrices <em>A</em>,<em>B</em>, and <em>C</em> are independent of the input since theirdimensions <strong><em>N</em></strong> and <strong><em>D</em></strong>are static and do not change.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e93b701-df74-4954-be5c-c0d83779d3df_1412x532.png" style="zoom:50%;" /></p><p>Instead, Mamba makes matrices <em>B</em> and <em>C,</em> and even the<em>step size</em> <strong>∆</strong><em>,</em> dependent on the inputby incorporating the sequence length and batch size of the input:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdccefffd-5712-45ab-9821-c794bce65d7d_1412x596.png" style="zoom:50%;" /></p><p>This means that for every input token, we now have different<em>B</em> and <em>C</em> matrices which solves the problem withcontent-awareness!</p><blockquote><p><strong>NOTE</strong>: Matrix <em>A</em> remains the same since wewant the state itself to remain static but the way it is influenced(through <em>B</em> and <em>C</em>) to be dynamic.</p></blockquote><p>Together, they <em>selectively</em> choose what to keep in the hiddenstate and what to ignore since they are now dependent on the input.</p><p>A smaller <em>step size</em> <strong>∆</strong> results in ignoringspecific words and instead using the previous context more whilst alarger <em>step size</em> <strong>∆</strong> focuses on the input wordsmore than the context:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06b21aab-aa32-450a-ae02-b976a2c9f9d8_2520x616.png" style="zoom:50%;" /></p><h3 id="the-scan-operation">The Scan Operation</h3><p>Since these matrices are now <em>dynamic</em>, they cannot becalculated using the convolution representation since it assumes a<em>fixed</em> kernel. We can only use the recurrent representation andlose the parallelization the convolution provides.</p><p>To enable parallelization, let’s explore how we compute the outputwith recurrency:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed3ad33b-7b7f-4b69-b28e-8437c7b71e2e_1540x536.png" style="zoom:50%;" /></p><p>Each state is the sum of the previous state (multiplied by<em>A</em>) plus the current input (multiplied by <em>B</em>). This iscalled a <em>scan operation</em> and can easily be calculated with a forloop.</p><p>Parallelization, in contrast, seems impossible since each state canonly be calculated if we have the previous state. Mamba, however, makesthis possible through the <em><ahref="https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda">parallelscan</a></em> algorithm.</p><p>It assumes the order in which we do operations does not matterthrough the associate property. As a result, we can calculate thesequences in parts and iteratively combine them:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F191fdabe-6b38-4e6b-a9f0-8240feef0a9d_1640x1100.png" style="zoom:50%;" /></p><p>Together, dynamic matrices <em>B</em> and <em>C</em>, and theparallel scan algorithm create the <strong><em>selective scanalgorithm</em></strong> to represent the dynamic and fast nature ofusing the recurrent representation.</p><h3 id="hardware-aware-algorithm">Hardware-aware Algorithm</h3><p>A disadvantage of recent GPUs is their limited transfer (IO) speedbetween their small but highly efficient SRAM and their large butslightly less efficient DRAM. Frequently copying information betweenSRAM and DRAM becomes a bottleneck.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a1d4fa3-526d-488e-8768-c7208f018eb4_1728x300.png" style="zoom:50%;" /></p><p>Mamba, like Flash Attention, attempts to limit the number of times weneed to go from DRAM to SRAM and vice versa. It does so through<em>kernel fusion</em> which allows the model to prevent writingintermediate results and continuously performing computations until itis done.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F563b3792-701a-42b1-9b68-7b6457f5e63d_1728x344.png" style="zoom:50%;" /></p><p>We can view the specific instances of DRAM and SRAM allocation byvisualizing Mamba’s base architecture:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F724eceb1-4356-4ac5-b44e-f7fabce3b472_1728x580.png" style="zoom:50%;" /></p><p>Here, the following are fused into one kernel:</p><ul><li><p>Discretization step with <em>step size</em><strong>∆</strong></p></li><li><p>Selective scan algorithm</p></li><li><p>Multiplication with <em>C</em></p></li></ul><p>The last piece of the hardware-aware algorithm is<em>recomputation</em>.</p><p>The intermediate states are not saved but are necessary for thebackward pass to compute the gradients. Instead, the authors recomputethose intermediate states <em>during</em> the backward pass.</p><p>Although this might seem inefficient, it is much less costly thanreading all those intermediate states from the relatively slow DRAM.</p><p>We have now covered all components of its architecture which isdepicted using the following image from its article:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc840fb8-2e24-4103-95c8-afa306ce0cfc_2409x743.png" style="zoom:50%;" /></p><p><strong>The Selective SSM.</strong> Retrieved from: Gu, Albert, andTri Dao. "Mamba: Linear-time sequence modeling with selective statespaces." <em>arXiv preprint arXiv:2312.00752</em> (2023).</p><p>This architecture is often referred to as a <strong><em>selectiveSSM</em></strong> or <strong><em>S6</em></strong> model since it isessentially an S4 model computed with the selective scan algorithm.</p><h3 id="the-mamba-block">The Mamba Block</h3><p>The <em>selective SSM</em> that we have explored thus far can beimplemented as a block, the same way we can represent self-attention ina decoder block.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb76db77-1dba-42bd-8de4-4bce240ff67e_1776x1012.png" style="zoom:50%;" /></p><p>Like the decoder, we can stack multiple Mamba blocks and use theiroutput as the input for the next Mamba block:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc94d349d-8620-45a9-8095-7c27de8b7865_1660x1356.png" style="zoom:50%;" /></p><p>It starts with a linear projection to expand upon the inputembeddings. Then, a convolution before the <em>Selective SSM</em> isapplied to prevent independent token calculations.</p><p>The <em>Selective SSM</em> has the following properties:</p><ul><li><p><em>Recurrent SSM</em> created through<em>discretization</em></p></li><li><p><em>HiPPO</em> initialization on matrix <em>A</em> to capture<em>long-range dependencies</em></p></li><li><p>S<em>elective scan algorithm</em> to selectively compressinformation</p></li><li><p><em>Hardware-aware algorithm</em> to speed upcomputation</p></li></ul><p>We can expand on this architecture a bit more when looking at thecode implementation and explore how an end-to-end example would looklike:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa67d7341-9a43-4c67-aa88-6e802c0902ae_1660x2040.png" style="zoom:50%;" /></p><p>Notice some changes, like the inclusion of normalization layers andsoftmax for choosing the output token.</p><p>When we put everything together, we get both fast inference andtraining and even unbounded context!</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe528e5fa-0dd8-4e6c-b31a-5248aaee6c68_2072x912.png" style="zoom:50%;" /></p><p>Using this architecture, the authors found it matches and sometimeseven exceeds the performance of Transformer models of the same size!</p><h2 id="conclusion"><strong>Conclusion</strong></h2><p>This concludes our journey in State Space Models and the incredibleMamba architecture using a selective State Space Model. Hopefully, thispost gives you a better understanding of the potential of State SpaceModels, particularly Mamba. Who knows if this is going to replace theTransformers but for now, it is incredible to see such differentarchitectures getting well-deserved attention!</p><h2 id="resources">Resources</h2><p>Hopefully, this was an accessible introduction to Mamba and StateSpace Models. If you want to go deeper, I would suggest the followingresources:</p><ul><li><a href="https://srush.github.io/annotated-s4/">The Annotated S4</a>is a JAX implementation and guide through the S4 model and is highlyadvised!</li><li>A great <ahref="https://www.youtube.com/watch?v=ouF-H35atOY">YouTube video</a>introducing Mamba by building it up through foundational papers.</li><li><a href="https://github.com/state-spaces/mamba">The Mambarepository</a> with <ahref="https://huggingface.co/state-spaces">checkpoints on HuggingFace</a>.</li><li>An amazing series of blog posts (<ahref="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-1">1</a>, <ahref="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-2">2</a>, <ahref="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-3">3</a>)that introduces the S4 model.</li><li>The <ahref="https://jameschen.io/jekyll/update/2024/02/12/mamba.html">MambaNo. 5 (A Little Bit Of...)</a> blog post is a great next step to diveinto more technical details about Mamba but still from an amazinglyintuitive perspective.</li><li>And of course, <a href="https://arxiv.org/abs/2312.00752">the Mambapaper</a>! It was even used for DNA modeling and speech generation.</li></ul><hr /><h2 id="references">References</h2><blockquote><p>[1]原Blog链接：https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state</p><p>[2] 一文通透想颠覆Transformer的Mamba：从SSM、HiPPO、S4到Mambahttps://blog.csdn.net/v_JULY_v/article/details/134923301</p><p>[2] Gu, A., &amp; Dao, T. (2023). Mamba: Linear-time sequencemodeling with selective state spaces. <em>arXiv preprintarXiv:2312.00752</em>.</p><p>[3]</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;mamba&quot;&gt;Mamba&lt;/h1&gt;
&lt;!--本文是根据Mamba论文原作者撰写的Blog进行阅读的重点解读--&gt;
&lt;p&gt;The Transformer architecture has been a major component in the
succes</summary>
      
    
    
    
    
    <category term="Deep Learning" scheme="http://junlei-zhou.com/tags/Deep-Learning/"/>
    
    <category term="Sequence Model" scheme="http://junlei-zhou.com/tags/Sequence-Model/"/>
    
    <category term="State space model" scheme="http://junlei-zhou.com/tags/State-space-model/"/>
    
  </entry>
  
  <entry>
    <title>Support-Vector-Machine</title>
    <link href="http://junlei-zhou.com/2024/05/29/Support-Vector-Machine/"/>
    <id>http://junlei-zhou.com/2024/05/29/Support-Vector-Machine/</id>
    <published>2024-05-29T07:56:16.000Z</published>
    <updated>2024-06-17T01:02:05.842Z</updated>
    
    <content type="html"><![CDATA[<h1 id="支持向量机-support-vector-machine">支持向量机 (Support VectorMachine)</h1><h2 id="简介">简介</h2><p>支持向量机是无监督机器学习中一个经典的算法，具有完善的数学理论和推理证明。本文将从SVM问题定义，模型建立，数学推理对SVM进行详细的论述。</p><h2 id="线性可分-linear-separable">线性可分 (Linear Separable)</h2><h4 id="问题定义">问题定义</h4><p>在一个样本空间中，给定训练样本集，在样本空间中能够找到一个划分超平面，将不同类别的样本区分开。</p><p><img src="https://imgur.la/images/2024/06/03/image-20240530093244843.png" alt="image 20240530093244843" border="0" style="zoom: 33%;" ></p><center><p>图1：线性可分，存在多个超平面将两类训练样本分开</p></center><p><strong>结论1：在一个空间中，如果存在一条直线能够划分两个点集，那么将存在无数条直线能够划分这两个点集。(证明见附录)</strong></p><h4 id="数学定义"><strong>数学定义</strong></h4><p><span class="math inline">\(D_1\)</span>和 <spanclass="math inline">\(D_2\)</span> 是 n 维欧氏空间中的两个点集。如果存在n 维向量 <span class="math inline">\(w\)</span> 和实数 <spanclass="math inline">\(b\)</span> 使得所有属于 <spanclass="math inline">\(D_0\)</span> 的点 <spanclass="math inline">\(x_i\)</span> 都有 <spanclass="math inline">\(wx_i+b&gt;0\)</span>, 而对于所有属于 <spanclass="math inline">\(D_1\)</span> 的点 <spanclass="math inline">\(x_j\)</span> 则有 <spanclass="math inline">\(wx_j+b&lt;0\)</span>，则我们称 <spanclass="math inline">\(D_0\)</span> 和 <spanclass="math inline">\(D_1\)</span> 线性可分。</p><h2 id="svm解决线性问题">SVM解决线性问题</h2><h4 id="线性问题定义">线性问题定义</h4><p>由 <strong>结论1</strong>可得，对于一个线性可分的问题，存在无数条满足要求的直线，那么究竟哪一条直线是最好的？</p><p><img src="https://imgur.la/images/2024/06/03/image-20240530104549138.png" alt="image-20240530104549138" style="zoom: 33%;" /></p><center><p>图2：线性可分，存在多个超平面将两类训练样本分开</p></center><p>直观上看，应该去找位于两类训练样本“正中间”的划分超平面，即图中红色的直线，因为该划分超平面对训练样本局部扰动的“容忍”性最好，例如，由于训练集的局限性或噪声的因素，训练集外的样本可能比图中的训练样本更接近两个类的分隔界，这将使许多划分超平面出现错误，而红色的超平面受影响最小，换言之，这个划分超平面所产生的分类结果是最鲁棒的，对未见示例的泛化能力最强。</p><p><strong>问题1：我们怎样定义这样一个最好的直线？</strong>答：为每条直线定义一个性能指标，将每条直线平移直至该直线能够插到最边缘的样本点，如下图所示：</p><p><img src="https://imgur.la/images/2024/06/03/image-20240530112035809.png" alt="image-20240530112035809" style="zoom: 33%;" /></p><center><p>图3：支持向量与间隔</p></center><p>如图3所示，我们做如下定义：</p><p><span class="math inline">\(r\)</span> :间隔(Margin)，将平行线平移直至插到支持向量的距离</p><p><span class="math inline">\(x\)</span> : 训练集中的样本</p><p><span class="math inline">\(y\)</span> : 训练集中样本对应的标签</p><p><span class="math inline">\(D=\{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\},y_i\in\{-1,+1\}\)</span> : 训练样本集</p><p><span class="math inline">\(w^Tx+b=0\)</span> :超平面(Hypeplane)，即上文中提到的直线，在二维空间中是一条直线，在高维空间中，则是超平面(hyperplane)。</p><h4 id="svm基本型">SVM基本型</h4><p>支持向量机——线性可分，本质为最大化 <strong>Margin</strong>问题，属于凸优化中的 <strong>二次规划</strong> 问题。</p><ul><li>最小化 (Minimize) : <spanclass="math inline">\(\frac{1}{2}||w||^2\)</span></li><li>限制条件 (Subject to) : <spanclass="math inline">\(y_i[w^Tx_i+b]\geq1, \ \ \ \ \ \ \ \ \ \ \ \i=1，2,\dots,n\)</span></li></ul><p><strong>解释：</strong></p><p><strong>定理1：</strong><span class="math inline">\(w^T+b=0\)</span>与 <span class="math inline">\(aw^T+ab=0\)</span> 是同一个超平面，若<span class="math inline">\((w,b)\)</span> 满足 <spanclass="math inline">\(y_i[w^Tx_i+b]\geq1\)</span> 那么 <spanclass="math inline">\((aw,ab)\)</span> 也满足 <spanclass="math inline">\(y_i[aw^Tx_i+ab]\geq1\)</span> 。</p><p><strong>定理2：</strong>点 <spanclass="math inline">\((x_0,y_0)\)</span> 到直线 <spanclass="math inline">\(ax+by+c=0\)</span> 的距离 <spanclass="math inline">\(d=\frac{|ax_0+by_0+c|}{\sqrt{a^2+b^2}}\)</span> ;点到超平面 <span class="math inline">\(H\)</span> 的距离即 <spanclass="math inline">\(r=\frac{|w^Tx_0+b|}{||w||}\)</span>.</p><p>由 <strong>定理1</strong>我们可以通过缩放将 <spanclass="math inline">\(|w^Tx_0+b|\)</span> 通过乘以一个缩放因子 <spanclass="math inline">\(a\)</span> 使得 <spanclass="math inline">\(|aw^Tx_0+ab|=1\)</span>, 则此时 <spanclass="math inline">\(r=\frac{1}{||w||}\)</span>, 故而最大化<strong>Margin</strong> 即可以转变为最小化 <spanclass="math inline">\(||w||\)</span>问题。由于我们在计算机中使用梯度下降进行最优化求解，为了求导数方便，因此常常使用<spanclass="math inline">\(||w||^2\)</span> 或者 <spanclass="math inline">\(\frac{1}{2}||w||^2\)</span> 的形式来代替 <spanclass="math inline">\(||w||\)</span> 。</p><p>限制条件 :即对于所有的样本点，除了支持向量，其他的点都必须比支持向量到超平面的距离要远。</p><p>到此，SVM的基本型便定义完成，这是一个简单的凸优化问题，我们可以使用简单的凸优化求解方法来实现计算(如梯度下降法)。</p><h2 id="svm解决非线性问题">SVM解决非线性问题</h2><h4 id="非线性可分">非线性可分</h4><p><img src="https://imgur.la/images/2024/06/05/SVM_image4_5.png" alt="SVM image4 5" border="0" style="zoom: 33%;" ></p><center><p>图4：非线性可分，在二维空间中，我们无法找到一条直线来分离这两个类别</p></center><p>如图4所示，对于非线性可分的情况，在二维空间中我们无法使用一条直线或者一个平面将两个类别完全区分。在低维空间中我们无法有效地将这两类点完全区分，但在高维空间中，我们可以找到一个超平面来完全区分这两类点(<ahref="https://en.wikipedia.org/wiki/Cover%27s_theorem"><strong>Cover’stheorem</strong></a>) .</p><p><img src="https://imgur.la/images/2024/06/03/SVM_image64b9b723ba732350b.png" alt="SVM image6" border="0" style="zoom: 25%;" ></p><center><p>图5：在低维空间中非线性可分，但将这些点映射到高维空间中则线性可分</p></center><p>因此，为了寻找一个超平面，我们需要将原本的样本点 <spanclass="math inline">\(x\)</span> 映射到一个高维空间中去，即设 <spanclass="math inline">\(\varphi(x)\)</span> 为映射函数，<spanclass="math inline">\(\varphi(x): \mathcal{X}\longrightarrow\mathcal{H}\)</span>，其中 <spanclass="math inline">\(\mathcal{X}\)</span> 为原输入空间，<spanclass="math inline">\(\mathcal{H}\)</span> 为特征空间( <spanclass="math inline">\(\mathcal{H}\)</span> 是Hibert Space，即完备的内积空间) <span class="math display">\[x \stackrel{\varphi}{\longrightarrow}\varphi(x)\]</span> <strong>问题2: 如何寻找这个高维映射方法？</strong> 直接将<span class="math inline">\(x\)</span> 映射为为无限维。</p><p>但此时由于 <span class="math inline">\(\varphi(x)\)</span>是无限维，无法计算。 此时我们使用 <strong>核技巧(kernel trick) :在输入空间中找到一个核函数(kernel function) <spanclass="math inline">\(K(x_1,x_2)\)</span> 使得 <spanclass="math inline">\(K(x_1,x_2)=&lt;\varphi(x_1),\varphi(x_2)&gt;\)</span>,从而可以直接在低维空间中计算出结果，加速核方法计算。</strong>(相关性质及定义请见 <strong>附录：核函数</strong>)</p><p><strong>问题3：如何实现非线性中完全区分不同类的点？</strong></p><p>为了使线性可分的SVM优化泛化到非线性可分的情况，我们允许支持向量机在一些样本上出错，为此引入了<strong>软间隔 (Soft Margin)</strong>，允许少部分样本可以不满足约束<span class="math inline">\(y_i[w^Tx_i+b]\geq1\)</span>,但在最大化间隔的同时，应该保证不满足约束的样本数量尽可能的少，因此优化目标进一步可以变为：<span class="math display">\[\mathop{\min}_{w,b}\  \frac{1}{2}||w||^2+C\sum\limits_{i=1}^{N}\mathscr{l}_{0/1}(y_i[w^Tx_i+b]-1)\]</span> 其中 <span class="math inline">\(C&gt;0\)</span>是一个常数，<span class="math inline">\(\mathscr{l}_{0/1}\)</span>是"0/1损失函数" <span class="math display">\[\mathscr{l}_{0/1}(z)= \left \{ \begin{array}{rcl} 1, &amp; \mbox{if}\ \z&lt;0; \\ 0, &amp; \mbox{otherwise.} \end{array} \right.\]</span> 其中，当 <span class="math inline">\(C\)</span>为无穷大时，新的优化目标式子能够迫使所有的样本均满足约束式子。此时 <spanclass="math inline">\(\frac{1}{2}||\omega||^2+C\sum\limits_{i=1}^{N}\mathscr{l}_{0/1}(y_i[w^Tx_i+b]-1)\)</span> 与 <spanclass="math inline">\(\frac{1}{2}||w||^2\)</span> 等价；当 <spanclass="math inline">\(C\)</span>为有限值时，新的优化目标允许一部分样本不满足约束。</p><p>然而，<span class="math inline">\(\mathscr{l}_{0/1}\)</span>非凸、非连续，数学性质不好，导致不易直接求解，故而我们通常使用其他的一些函数代替<span class="math inline">\(\mathscr{l}_{0/1}\)</span> 并称之为"替代损失 (surrogateloss)"。这些替代损失通常具有较好的数学性质，通常是凸的连续函数且是 <spanclass="math inline">\(\mathscr{l}_{0/1}\)</span>的上界。如我们采用hinge损失代替 <spanclass="math inline">\(\mathscr{l}_{0/1}\)</span> 则优化目标可以变为<span class="math display">\[\mathop{\min}_{w,b}\  \frac{1}{2}||\omega||^2+C\sum\limits_{i=1}^{N}\max(0,\ 1-y_i[w^Tx_i+b])\]</span> 实际中我们可以引入<strong>松弛变量 (Slack Variable) <spanclass="math inline">\(\xi_i\geq0\)</span></strong>来代替上式中的替代损失，得到 <span class="math display">\[\mathop{\min}_{w,b}\ \frac{1}{2}||\omega||^2+C\sum\limits_{i=1}^{N}\xi_i\]</span>从机器学习的角度来看，非线性SVM的优化目标可以看作为对线性的SVM优化目标添加了一个<strong>正则项 (Regulation) <span class="math inline">\(C\sum\limits_{i=1}^{N}\xi_i\)</span></strong></p><h4 id="非线性svm优化">非线性SVM优化</h4><p>支持向量机——非线性可分 ("软间隔"支持向量机)</p><ul><li>最小化 (Minimize) : <spanclass="math inline">\(\frac{1}{2}||w||^2+C\sum\limits_{i=1}^{N}\xi_i\)</span></li><li>限制条件 (Subject to) :<ul><li>​ <span class="math inline">\(y_i[w^T\varphi(x_i)+b]\geq1-\xi_i, \ \\ \ \ \ \ \ \ \ \ \ i=1，2,\dots,n\)</span></li><li>​ <span class="math inline">\(\xi_i\geq0\)</span></li></ul></li></ul><p>在这个式子中存在高维映射 <spanclass="math inline">\(\varphi(x_i)\)</span>,因而无法直接求解，因此我们要将该优化问题转换为可以求解的形式。</p><h4 id="解非线性svm的优化问题">解非线性SVM的优化问题</h4><p><strong>关键：将非线性的SVM优化问题转换为其对应的对偶问题，利用求其对偶问题的解来代替求原非线性SVM问题的解</strong></p><p>(原问题与对偶问题的关系证明见附录——优化理论：原问题与对偶问题)</p><ul class="task-list"><li><p><label><input type="checkbox" /><strong>Step 1:</strong>为了方便转换为其对应的对偶问题，首先要转变原问题的形式，使其与附录中的原问题的形式相一致。</label></p><p><img src="https://imgur.la/images/2024/06/05/SVM_image7.png" alt="SVM image7" border="0"></p><center><p><p>图6：将非线性SVM的优化问题形式转换，与原问题中的形式保持一致</p></p></center><p><strong>解释：</strong>其中 <spanclass="math inline">\(\xi_i\)</span> 是松弛变量，我们调整其取值范围为<span class="math inline">\(\xi_i\leq0\)</span>,相比较于原式子，只是在原来的 <span class="math inline">\(\xi_i\)</span>前面加了一个 "负号"。因此保证之前的优化目标不变，故而要变为 <spanclass="math inline">\(\frac{1}{2}||w||^2-C\sum\limits_{i=1}^{N}\xi_i\)</span> ,限制条件变为 <spanclass="math inline">\(y_i[w^T\varphi(x_i)+b]\geq1+\xi_i\)</span> ,将其移项得 <spanclass="math inline">\(1+\xi_i-y_iw^T\varphi(x_i)-y_ib\leq0\)</span>。此时原问题中的 <span class="math inline">\(f(w)\Longrightarrow\frac{1}{2}||\omega||^2-C\sum \limits_{i=1}^{N}\xi_i\)</span> , <spanclass="math inline">\(g_i(w)\leq0 \Longrightarrow1+\xi_i-y_iw^T\varphi(x_i)-y_ib\leq0 和 \xi_i\leq0\)</span> 。</p><h5 id="非线性svm优化-转换后">非线性SVM优化 (转换后)</h5><p>支持向量机——非线性可分 ("软间隔"支持向量机)</p><ul><li>最小化 (Minimize) : <spanclass="math inline">\(\frac{1}{2}||\omega||^2-C\sum\limits_{i=1}^{N}\xi_i\)</span></li><li>限制条件 (Subject to) :<ul><li>​ <span class="math inline">\(1+\xi_i-y_iw^T\varphi(x_i)-y_ib\leq0, \\ \ \ \ \ \ \ \ \ \ \ i=1，2,\dots,n\)</span></li><li>​ <span class="math inline">\(\xi_i\leq0\)</span></li></ul></li></ul></li><li><p><label><input type="checkbox" /><strong>Step 2: </strong>寻找转换后的非线性SVM优化问题的对偶问题，根据<strong>附录——优化理论：原问题与对偶问题</strong>我们可得其对偶问题为：</label></p><h5 id="非线性svm优化问题的对偶问题">非线性SVM优化问题的对偶问题</h5><ul><li>最大化 (Maximum) : <spanclass="math inline">\(\theta(\alpha,\beta)=\inf\limits_{w,\xi_i,b}\{\frac{1}{2}||\omega||^2-C\sum\limits_{i=1}^N\xi_i+\sum\limits_{i=1}^{N}\alpha_i[1+\xi_i-y_iw^T\varphi(x_i)-y_ib]+\sum\limits_{i=1}^{N}\beta_i\xi_i\}\)</span></li><li>限制条件 (Subject to) : <spanclass="math inline">\(i=1,2,\dots,N\)</span><ul><li><span class="math inline">\(\alpha_i\geq0\)</span> or <spanclass="math inline">\(\alpha\succeq0\)</span></li><li><span class="math inline">\(\beta_i\geq0\)</span> or <spanclass="math inline">\(\beta\succeq0\)</span></li></ul></li></ul><p><strong>解释：</strong>其中优化目标中的 <spanclass="math inline">\(\alpha\)</span> 和 <spanclass="math inline">\(\beta\)</span> 与<strong>附录——优化理论：原问题与对偶问题</strong>中的拉格朗日乘子中稍微有些不同，在原拉格朗日乘子中<span class="math inline">\(\alpha\)</span> 控制限制条件中不等式， <spanclass="math inline">\(\beta\)</span>控制限制条件中的等式。由于我们转换后的非线性SVM优化问题的限制条件中不存在等式<span class="math inline">\(h_i(w)=0\)</span>这一项，故而非线性SVM优化问题对应的拉格朗日乘子 <spanclass="math inline">\(L(w,\alpha,\beta)=f(w)+\sum\limits_{i=1}^{K}\alpha_ig_i(w)+\sum\limits_{i=1}^{M}\beta_ih_i(w)\)</span> 中不存在 <spanclass="math inline">\(\sum \limits_{i=1}^{M}\beta_ih_i(w)\)</span>项。而我们转换后的非线性SVM优化问题的限制条件中存在两个不等式，故而非线性SVM优化问题对应的拉格朗日乘子包含了两个控制不等式的<span class="math inline">\(\alpha\)</span>,这里是为了在同一个式子中便于区分故而分别写为了 <spanclass="math inline">\(\alpha,\beta\)</span>, 我们转化后的对偶问题中的<span class="math inline">\(\alpha,\beta\)</span> 都对应拉格朗日乘子中的<span class="math inline">\(\alpha\)</span>. 对于限制条件中 <spanclass="math inline">\(\alpha_i\geq0\)</span> 与 $$0的写法不同但含义相同，只不过前者表示向量 <spanclass="math inline">\(\alpha\)</span> 中的每一项都大于0，而后者表示向量<span class="math inline">\(\alpha\)</span> 大于0.</p></li><li><p><label><input type="checkbox" /><strong>Step 3:</strong>求对偶问题的解，即要求出一组 <spanclass="math inline">\(w,\xi_i,b\)</span> 使优化目标中 <spanclass="math inline">\(\{\cdots\}\)</span> 部分最小。</label></p><p>利用求导法，令 <span class="math display">\[\begin{align}&amp;\frac{\partial L}{\partial\omega}=0 \ \ \ \Longrightarroww-\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i)=0 \ \ \ \Longrightarroww=\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i) \\&amp;\frac{\partial L}{\partial\xi_i}=0 \ \ \ \Longrightarrow\alpha_i+\beta_i=C \ \ \ \Longrightarrow \alpha_i+\beta_i=C \\&amp;\frac{\partial L}{\partial b}=0 \ \ \ \Longrightarrow\sum_{i=1}^{N}\alpha_iy_i=0\end{align}\]</span> 将上式代入 <spanclass="math inline">\(\theta(\alpha,\beta)\)</span> 得： <spanclass="math display">\[\begin{align}\theta(\alpha,\beta)&amp;=\inf \limits_{w,\xi_i,b}\{\frac{1}{2}||w||^2-C\sum\limits_{i=1}^N\xi_i+\sum_{i=1}^{N}\alpha_i[1+\xi_i-y_iw^T\varphi(x_i)-y_ib]+\sum_{i=1}^{N}\beta_i\xi_i\} \\&amp;=\inf \limits_{w,\xi_i,b}\{\frac{1}{2}w^Tw-\bcancel{\sum_{i=1}^{N}\alpha_i\xi_i}-\bcancel{\sum_{i=1}^{N}\beta_i\xi_i}+\sum_{i=1}^{N}\alpha_i+\bcancel{\sum_{i=1}^{N}\alpha_i\xi_i}-\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i)w^T-\bcancel{\sum_{i=1}^{N}\alpha_iy_ib}+\bcancel{\sum_{i=1}^{N}\beta_i\xi_i}\} \\&amp;=\inf \limits_{w,\xi_i,b}\{\frac{1}{2}\big(\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i)\big)^T\big(\sum_{j=1}^{N}\alpha_jy_j\varphi(x_j)\big)+\sum_{i=1}^{N}\alpha_i-\big(\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i)\big)^T\big(\sum_{j=1}^{N}\alpha_jy_j\varphi(x_j)\big)\} \\&amp;=\inf \limits_{w,\xi_i,b}\{\sum_{i=1}^{N}\alpha_i-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\underbrace{\alpha_i\alpha_j}_{常数}\underbrace{y_iy_j}_{y=\pm1}\underbrace{\varphi(x_i)^T\varphi(x_j)}_{K(x_i,x_j)}\}\end{align}\]</span> 最后，对偶问题可以转换为：</p><h5 id="非线性svm优化问题的对偶问题-求解后">非线性SVM优化问题的对偶问题(求解后)</h5><ul><li>最大化 (Maximum) : <spanclass="math inline">\(\theta(\alpha)=\sum\limits_{i=1}^{N}\alpha_i-\frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i\alpha_j\underbrace{y_jy_i}_{已知}\underbrace{K(x_i,x_j)}_{已知}\)</span></li><li>限制条件 (Subject to) : <spanclass="math inline">\(i=1,2,\dots,N\)</span><ul><li><span class="math inline">\(\alpha\geq0, \&amp;\&amp;\  \beta_i\geq0\ \ \ \Longrightarrow\ 0\leq\alpha_i\leq C\)</span></li><li><spanclass="math inline">\(\sum\limits_{i=1}^{N}\alpha_iy_i=0\)</span></li></ul></li></ul><p>此时，该问题变为了一个基本的凸优化问题，我们可以使用 <ahref="https://en.wikipedia.org/wiki/Sequential_minimal_optimization"><strong>SMO算法</strong></a>对其进行求解。</p></li><li><p><label><input type="checkbox" /><strong>Step 4: </strong>将求<span class="math inline">\(\alpha\)</span> 转化为求 <spanclass="math inline">\(w,b\)</span>.</label></p><p>由 <strong>Step 3</strong> 有 <spanclass="math inline">\(w=\sum\limits_{i=1}^{N}\alpha_iy_i\varphi(x_i)\)</span>，但其中仍然存在高维向量 <spanclass="math inline">\(\varphi(x_i)\)</span>似乎无法求解，但实际上，我们对于一个测试样本 <spanclass="math inline">\(x\)</span> ,则有: <span class="math display">\[\left\{ \begin{array}{rcl}若w^T\varphi(x)+b\geq0,则\ y=+1\\若w^T\varphi(x)+b&lt;0,则\ y=-1\end{array}\right.\]</span> 其中 <spanclass="math inline">\(w^T\varphi(x)=\sum\limits_{i=1}^{N}\alpha_iy_i\varphi(x_i)^T\varphi(x_i)\\  \Longrightarrow \sum\limits_{i=1}^{N}\alpha_iy_iK(x_i,x)\)</span> ,即我们不需要知道 <span class="math inline">\(w\)</span>具体是多少，我们可以直接算出 <spanclass="math inline">\(w^T\varphi(x)\)</span> 的值。</p><p>现在只需要求出 <span class="math inline">\(b\)</span> 即可，利用<strong>KKT条件</strong> 求 <span class="math inline">\(b\)</span>.</p><p><strong>KKT条件：</strong>对于 <span class="math inline">\(\foralli=1,2,\dots,K.\)</span> 则有 <spanclass="math inline">\(\alpha_i^*=0\)</span> 或者 <spanclass="math inline">\(g_i(w^*)=0\)</span> .</p><p><strong>转化为SVM中的KKT</strong>，则有 <spanclass="math inline">\(\forall\  i=1,2,\dots,N\)</span></p><p><span class="math inline">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\ \ \ \blacktriangleright\)</span> 要么 $ _i=0$, 要么 <spanclass="math inline">\(\xi_i=0\)</span></p><p><span class="math inline">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\ \ \ \blacktriangleright\)</span> 要么 <spanclass="math inline">\(\alpha_i=0\)</span>, 要么 <spanclass="math inline">\(1+\xi_i-y_iw^T\varphi(x_i)-y_ib=0\)</span></p><p>因此，我们随机选取一个 <span class="math inline">\(\alpha_i\)</span>, 则 <span class="math inline">\(0&lt;\alpha_i&lt;C\)</span>， 故而<span class="math inline">\(\beta_i=C-\alpha_i&gt;0\)</span></p><p><span class="math inline">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\therefore \beta_i\neq0, \ 则\ \xi_i=0\)</span>,</p><p><span class="math inline">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\therefore \alpha_i\neq0, \ 则 \1+\xi_i-y_iw^T\varphi(x_i)-y_ib=0\)</span></p><p><span class="math inline">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\thereforeb=\frac{1}{y_i}-\sum_{i=1}^{N}\alpha_iy_iK(x_i,x)\)</span></p><p>至此，非线性SVM的优化问题求解完毕！</p></li></ul><h2 id="总结">总结</h2><h4 id="svm算法训练过程">SVM算法：训练过程</h4><p>输入 <spanclass="math inline">\(\{(x_i,y_i)\}_{i=1,2,\dots,N}\)</span>，解优化问题</p><ul><li>最大化 (Maximum) : <spanclass="math inline">\(\theta(\alpha)=\sum\limits_{i=1}^{N}\alpha_i-\frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i\alpha_j\underbrace{y_jy_i}_{已知}\underbrace{K(x_i,x_j)}_{已知}\)</span></li><li>限制条件 (Subject to) : <spanclass="math inline">\(i=1,2,\dots,N\)</span><ul><li><span class="math inline">\(\alpha\geq0, \&amp;\&amp;\  \beta_i\geq0\ \ \ \Longrightarrow\ 0\leq\alpha_i\leq C\)</span></li><li><spanclass="math inline">\(\sum\limits_{i=1}^{N}\alpha_iy_i=0\)</span></li></ul></li></ul><p>利用SMO算法求解最优化问题，得出 <spanclass="math inline">\(\alpha\)</span>, 下一步寻找一个 <spanclass="math inline">\(0&lt;\alpha_i&lt;0\)</span>，计算 <spanclass="math inline">\(b\)</span> :</p><p>​ <spanclass="math inline">\(b=\frac{1}{y_i}-\sum_{i=1}^{N}\alpha_iy_iK(x_i,x)\)</span></p><h4 id="svm算法测试过程">SVM算法：测试过程</h4><p>输入样本 <span class="math inline">\(x\)</span>：</p><ul><li>若 <spanclass="math inline">\(\sum\limits_{i=1}^{N}\alpha_iy_iK(x_i,x)+b\geq0\)</span>,则 $ y=+1$,</li><li>若 <spanclass="math inline">\(\sum\limits_{i=1}^{N}\alpha_iy_iK(x_i,x)+b&lt;0\)</span>,则 $ y=+1$.</li></ul><h2 id="附录">附录</h2><h4 id="结论1证明">结论1证明：</h4><h5 id="二维平面上的证明">二维平面上的证明</h5><p>假设我们有两个点集 <span class="math inline">\(A\)</span> 和 <spanclass="math inline">\(B\)</span>，并且这两个点集是线性可分的。这意味着存在一条直线可以将这两个点集分开。为了证明存在无数条直线能够划分这两个点集，我们可以如下进行证明：</p><ol type="1"><li><p><strong>存在一条直线</strong>：假设存在一条直线 <spanclass="math inline">\(L\)</span> 可以将点集 <spanclass="math inline">\(A\)</span> 和 <spanclass="math inline">\(B\)</span> 分开。直线 <spanclass="math inline">\(L\)</span> 可以表示为： <spanclass="math display">\[𝑦=k𝑥+b\]</span> 其中 <span class="math inline">\(k\)</span> 是斜率，<spanclass="math inline">\(b\)</span> 是截距。</p></li><li><p><strong>平行直线的性质</strong>：对于任何一个固定的斜率𝑚<em>m</em>，不同的截距 𝑐<em>c</em>会产生不同的平行直线。设我们有另一条直线 𝐿′<em>L</em>′ 其方程为： <spanclass="math display">\[𝑦=𝑚𝑥+b^′\]</span> 其中 <span class="math inline">\(b&#39;\neq b\)</span>，则<span class="math inline">\(L&#39;\)</span> 与 <spanclass="math inline">\(L\)</span> 平行。</p></li><li><p><strong>平行直线的分离能力</strong>：由于 <spanclass="math inline">\(A\)</span> 和 <spanclass="math inline">\(B\)</span> 是线性可分的，这意味着可以找到一个间隔<span class="math inline">\(\epsilon\)</span> 使得在直线 <spanclass="math inline">\(L\)</span>的一侧存在一个空隙，其中没有任何点在此区域。换句话说，我们可以调整<span class="math inline">\(b\)</span> 的值，使得新的直线 <spanclass="math inline">\(L&#39;\)</span> 依然可以将 <spanclass="math inline">\(A\)</span> 和 <spanclass="math inline">\(B\)</span> 分开。</p></li><li><p><strong>无数条平行直线</strong>：由于 <spanclass="math inline">\(b\)</span>可以在实数范围内任意取值，因此存在无数个不同的 <spanclass="math inline">\(b\)</span> 值，这意味着存在无数条平行直线可以将<span class="math inline">\(A\)</span> 和 <spanclass="math inline">\(B\)</span> 分开。</p></li><li><p><strong>非平行直线</strong>：除了平行直线之外，我们还可以选择不同的斜率<span class="math inline">\(k&#39;\)</span>。对任何新的斜率 <spanclass="math inline">\(k&#39;\)</span>，只要新的直线方程能够满足分离点集的条件，我们就可以调整其截距<span class="math inline">\(b&#39;\)</span> 使得新的直线 <spanclass="math inline">\(y=k&#39;x+b&#39;\)</span> 依然可以将 <spanclass="math inline">\(A\)</span> 和 <spanclass="math inline">\(B\)</span> 分开。</p></li></ol><h5 id="高维空间的推广">高维空间的推广</h5><p>在高维空间中，能够分离点集的“直线”实际上是一个超平面。假设我们有两个点集<span class="math inline">\(A\)</span> 和 <spanclass="math inline">\(B\)</span>，并且它们是线性可分的，即存在一个超平面<span class="math inline">\(H\)</span>可以将它们分开。超平面方程可以表示为： <span class="math display">\[w\cdot x+b=0\]</span> 其中 <span class="math inline">\(w\)</span> 是法向量，<spanclass="math inline">\(b\)</span> 是偏置项。</p><ol type="1"><li><strong>存在一个超平面</strong>：假设存在一个超平面 <spanclass="math inline">\(H\)</span> 可以将 <spanclass="math inline">\(A\)</span> 和 <spanclass="math inline">\(B\)</span> 分开。</li><li><strong>平行超平面</strong>：对于任何固定的法向量 <spanclass="math inline">\(w\)</span>，不同的偏置 <spanclass="math inline">\(b\)</span> 会产生不同的平行超平面。</li><li><strong>无数个平行超平面</strong>：由于 <spanclass="math inline">\(b\)</span>可以在实数范围内任意取值，因此存在无数个不同的 <spanclass="math inline">\(b\)</span> 值，这意味着存在无数个平行超平面可以将𝐴<em>A</em> 和 𝐵<em>B</em> 分开。</li><li><strong>非平行超平面</strong>：我们还可以选择不同的法向量 <spanclass="math inline">\(w&#39;\)</span>。对于任何新的法向量 <spanclass="math inline">\(w&#39;\)</span>，只要新的超平面方程能够满足分离点集的条件，我们就可以调整其偏置<span class="math inline">\(b\)</span> 使得新的超平面 <spanclass="math inline">\(w&#39;\cdot x+b&#39;=0\)</span> 依然可以将 <spanclass="math inline">\(A\)</span> 和 <spanclass="math inline">\(B\)</span> 分开。</li></ol><h4 id="核函数">核函数</h4><ul><li><p>在实际应用时，映射函数 <spanclass="math inline">\(\varphi(x)\)</span>不需要是已知的。换句话说，核技巧的目的就是在不需要显式地定义特征空间和映射函数的条件下，计算映射之后的内积</p></li><li><p>核函数 <spanclass="math inline">\(K(x_1,x_2)=&lt;\varphi(x_1),\varphi(x_2)&gt;\)</span>需要满足的充要条件 (不需要 <spanclass="math inline">\(\varphi(x)\)</span> 已知) (<ahref="%5BMercer&#39;s%20theorem%20-%20Wikipedia%5D(https://en.wikipedia.org/wiki/Mercer&#39;s_theorem)">Mercer'sTheorem</a>):</p><ul><li><p>对称性: <spanclass="math inline">\(K(x_1,x_2)=K(x_2,x_1)\)</span></p></li><li><p>半正定性: 对于任意 <span class="math inline">\(n\)</span> 和任意<span class="math inline">\(x_1,x_2,\dots,x_n∈\mathcal{X}\)</span>，由<span class="math inline">\(K(x_i,x_j)\)</span> 定义的 Gram matrix总是半正定的，即 <span class="math display">\[对于\forall C_i,x_i, (i=1，2,\dots,n),有\sum_{i=1}^N\sum_{j=1}^NC_iC_jK(x_1,x_2)\geq0\ \ \ \ \ \ \ \ \ \ \ \ \ \\ C_i为任意实数，x_i为任意的向量.\]</span></p></li></ul></li><li><p>只要 <span class="math inline">\(K\)</span>是核函数，那么一定存在一个Hilbert space和一个映射函数 <spanclass="math inline">\(\varphi(x)\)</span>，使得 <spanclass="math inline">\(K(x_1,x_2)=&lt;\varphi(x_1),\varphi(x_2)&gt;\)</span></p></li><li><p>常见核函数</p><table><colgroup><col style="width: 9%" /><col style="width: 47%" /><col style="width: 42%" /></colgroup><thead><tr class="header"><th>名称</th><th>表达式</th><th>参数</th></tr></thead><tbody><tr class="odd"><td>线性核</td><td><span class="math inline">\(K(x_1,x_2)=x_i^Tx_j\)</span></td><td></td></tr><tr class="even"><td>多项式核</td><td><span class="math inline">\(K(x_1,x_2)=(x_i^Tx_j)^d\)</span></td><td><span class="math inline">\(d\geq1\)</span>为多项式的次数</td></tr><tr class="odd"><td>高斯核</td><td><spanclass="math inline">\(K(x_1,x_2)=\exp(-\frac{||x_i-x_j||^2}{2\sigma^2})\)</span></td><td><span class="math inline">\(\sigma&gt;0\)</span>，为高斯核的带宽(width)</td></tr><tr class="even"><td>拉普拉斯核</td><td><spanclass="math inline">\(K(x_1,x_2)=\exp(-\frac{||x_i-x_j||}{\sigma})\)</span></td><td><span class="math inline">\(\sigma&gt;0\)</span></td></tr><tr class="odd"><td>Sigmoid核</td><td><span class="math inline">\(K(x_1,x_2)=\tanh(\betax_i^Tx_j+\theta)\)</span></td><td><span class="math inline">\(\tanh\)</span> 为双曲正切函数，<spanclass="math inline">\(\beta&gt;0,\ \ \theta&lt;0\)</span></td></tr></tbody></table><h4 id="替代损失函数">替代损失函数</h4><table><colgroup><col style="width: 44%" /><col style="width: 55%" /></colgroup><thead><tr class="header"><th>名称</th><th>表达式</th></tr></thead><tbody><tr class="odd"><td>hinge损失函数</td><td><spanclass="math inline">\(\mathscr{l}_{hinge}(z)=\max(0,1-z)\)</span></td></tr><tr class="even"><td>指数损失函数 (exponential loss)</td><td><spanclass="math inline">\(\mathscr{l}_{exp}(z)=\exp(-z)\)</span></td></tr><tr class="odd"><td>对率损失 (logistic loss)</td><td><spanclass="math inline">\(\mathscr{l}_{log}(z)=\log(1+\exp(-z))\)</span></td></tr></tbody></table><p><img src="https://imgur.la/images/2024/06/04/surrogate_loss.png" alt="surrogate loss" border="0" style="zoom: 5%;" ></p><center><p><p>图7：三种常见的替代损失函数: hinge损失、指数损失、对率损失</p></p></center></li></ul><h4 id="优化理论原问题与对偶问题">优化理论：原问题与对偶问题</h4><p><strong>1. 原问题 (Prime Problem)</strong></p><ul><li>最小化 (Minimize) : <span class="math inline">\(f(w)\)</span></li><li>限制条件 (Subject to) :<ul><li><span class="math inline">\(g_i(w)\leq0 \ \ \ \(i=1,2,\dots,K)\)</span></li><li><span class="math inline">\(h_i(w)=0 \ \ \ \(i=1,2,\dots,M)\)</span></li></ul></li></ul><p>对原问题使用拉格朗日乘子法可以得到其"对偶问题" <spanclass="math display">\[\begin{align}&amp;L(w,\alpha,\beta) \\=&amp;f(w)+\sum \limits_{i=1}^{K}\alpha_ig_i(w)+\sum\limits_{i=1}^{M}\beta_ih_i(w) \\=&amp;f(w)+\alpha^Tg(w)+\beta^Th(w)\end{align}\]</span> 根据您上述过程，我们可以得到原问题的对偶问题：</p><p><strong>2. 对偶问题 (Dual Problem)</strong></p><ul><li>最大化 (Maximum) : <spanclass="math inline">\(\theta(\alpha,\beta)=\inf\{L(w,\alpha,\beta)\}\)</span></li><li>限制条件 (Subject to) :<ul><li><span class="math inline">\(\alpha_i\geq0 \ \ \ \(i=1,2,\dots,K)\)</span></li></ul></li></ul><p><strong>解释：</strong></p><p>其中 <span class="math inline">\(\inf\{L(w,\alpha,\beta)\}\)</span>表示在限定的 <span class="math inline">\(\alpha,\beta\)</span>的条件下，遍历所有的 <span class="math inline">\(w\)</span>，求 <spanclass="math inline">\(L(w,\alpha,\beta)\)</span>的最小值。因此对于每确定的一组 <spanclass="math inline">\(\alpha,\beta\)</span> 都可以算出 <spanclass="math inline">\(L(w,\alpha,\beta)\)</span>的最小值。而我们此时要找的就是所有的能算出 <spanclass="math inline">\(L(w,\alpha,\beta)\)</span> 最小值的 <spanclass="math inline">\(\alpha,\beta\)</span> 中最大的那一组 <spanclass="math inline">\(\alpha,\beta\)</span>。</p><p><strong>3. 将原问题的解转化为对偶问题的解</strong></p><p><strong>定理：</strong>如果 <spanclass="math inline">\(w^*\)</span>是原问题的解，而<spanclass="math inline">\(\alpha^*,\beta^*\)</span>是对偶问题的解，则有<spanclass="math inline">\(f(w^*)\geq\theta(\alpha^*,\beta^*)\)</span></p><p>证明：(其中 <span class="math inline">\(\alpha^*\geq0, \ \g_i(w^*)\leq0, \ \ h_i(w)=0\)</span>) <span class="math display">\[\begin{align}\theta(\alpha^*,\beta^*)&amp;=\inf\{L(w,\alpha^*,\beta^*)\} \\&amp;\leq L(w^*,\alpha^*,\beta^*) \\&amp;=f(w^*)+\sum \limits_{i=1}^{K}\alpha_i^*g_i(w^*)+\sum\limits_{i=1}^{M}\beta_i^*h_i(w^*) \\&amp;\leq f(w^*)\end{align}\]</span> <strong>定义：</strong><spanclass="math inline">\(G=f(\omega^*)-\theta(\alpha^*,\beta^*)\geq0\)</span>,<span class="math inline">\(G\)</span> 为原问题与对偶问题的间距 (DualityGap)。</p><p><strong>强对偶定理：</strong>若 <spanclass="math inline">\(f(w)\)</span> 为凸函数，且 <spanclass="math inline">\(g(w)=aw+b, \ \ h(w)=cw+d\)</span>(即其约束条件均为线性函数), 则此优化问题得原问题与对偶问题的对偶间距<span class="math inline">\(G=0\)</span>.</p><p>由强对偶性定理得： <span class="math display">\[f(w^*)=\theta(\alpha^*,\beta^*)\]</span> 此时意味着，对于 <span class="math inline">\(\foralli=1,2,\dots,K.\)</span> 则有 <spanclass="math inline">\(\alpha_i^*=0\)</span> 或者 <spanclass="math inline">\(g_i(w^*)=0\)</span> .<strong>——KKT条件</strong></p><hr /><h2 id="references">References</h2><blockquote><p>[1] 周志华，机器学习，清华大学出版社，2016</p><p>[2] Boyd S P, Vandenberghe L. Convex optimization[M]. Cambridgeuniversity press, 2004.</p><p>[3] Cortes C, Vapnik V. Support-vector networks[J]. Machine learning,1995, 20: 273-297.</p><p>[4] Ruszczyński A P. Nonlinear optimization[M]. Princeton universitypress, 2006.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;支持向量机-support-vector-machine&quot;&gt;支持向量机 (Support Vector
Machine)&lt;/h1&gt;
&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;支持向量机是无监督机器学习中一个经典的算法，具有完善的数学理论和推理证明。本文将</summary>
      
    
    
    
    
    <category term="Machine Learning" scheme="http://junlei-zhou.com/tags/Machine-Learning/"/>
    
    <category term="Supervised machine learning" scheme="http://junlei-zhou.com/tags/Supervised-machine-learning/"/>
    
    <category term="Convex optimization" scheme="http://junlei-zhou.com/tags/Convex-optimization/"/>
    
    <category term="Dural Problem" scheme="http://junlei-zhou.com/tags/Dural-Problem/"/>
    
  </entry>
  
  <entry>
    <title>markov_process</title>
    <link href="http://junlei-zhou.com/2023/12/27/markov-process/"/>
    <id>http://junlei-zhou.com/2023/12/27/markov-process/</id>
    <published>2023-12-27T09:33:42.000Z</published>
    <updated>2024-01-17T10:28:21.516Z</updated>
    
    <content type="html"><![CDATA[<h2 id="马尔可夫过程-state-probability">马尔可夫过程 (State +Probability)</h2><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Markov_process-example.svg/2880px-Markov_process-example.svg.png" alt="undefined" style="zoom:50%;" /></p><h3 id="状态state">状态（State）</h3><p><strong>定义：</strong>具有<strong>马尔可夫性质</strong>的有限随机状态序列<strong><span class="math inline">\(S_1, S_2, ......\)</span></strong></p><p>马尔可夫性质（markovproperty）：当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；</p><p><span class="math inline">\(P(S_{t+1} | S_t) = P(S_{t+1}|S_1, ...,S_t)\)</span> <strong>无后效性</strong></p><p>解释：从<span class="math inline">\(S_1, ...,S_t\)</span>所蕴含的信息与<spanclass="math inline">\(S_t\)</span>是等价的，当前的状态只由上一个状态所决定，而不受更早之前状态影响。</p><h3 id="转移概率transition-probability">转移概率（TransitionProbability）</h3><p>状态转移矩阵</p><p><img src="C:\Users\S\AppData\Roaming\Typora\typora-user-images\image-20231228110315549.png" alt="image-20231228110315549" style="zoom:50%;" /><img src="C:\Users\S\AppData\Roaming\Typora\typora-user-images\image-20231228110454720.png" alt="image-20231228110454720" style="zoom:50%;" /></p><h2 id="马尔可夫奖励过程state-probabilityreward">马尔可夫奖励过程（State+ Probability+Reward）</h2><p>马尔可夫过程+奖励=马尔可夫奖励过程</p><p>奖励：</p><p>即时奖励：<span class="math inline">\(R_{t+1}\)</span></p><p>其中即时奖励与状态转移相伴随： <spanclass="math inline">\(R_{t+1}=f(S_t \rightarrow S_{t+1})\)</span></p><p>只要发生状态转移就会产生即时奖励。</p><p>长期奖励：<span class="math inline">\(G_t\)</span></p><p><span class="math inline">\(G_t = R_{t+1}+\gamma R_{t+2}+\gamma ^2R_{t+3}+\dots=\sum \limits_{k=0}^{\infty}\gamma^kR_{t+k+1}\)</span></p><p><span class="math inline">\(\gamma:\)</span>衰减系数，表示即时奖励在当前的折扣值， <spanclass="math inline">\(\gamma \in [0,1]\)</span></p><p><span class="math inline">\(\gamma\)</span> 越接近0，更喜欢现在的reward；<span class="math inline">\(\gamma\)</span>越接近1， 越不关心现在已经得到的reward。</p><p>长期奖励与状态转移链相伴随， 不同的状态转移链对应不同的长期奖励。</p><p>价值函数：<span class="math inline">\(V(s)\)</span></p><p>评价状态 <span class="math inline">\(S\)</span>的质量，使用以当前状态 <span class="math inline">\(S\)</span>起点的所有状态转移链的 <span class="math inline">\(G_t\)</span>的期望来作为衡量 <span class="math inline">\(S\)</span> 的价值指标 <spanclass="math display">\[\begin{align}V(S)&amp;=E[G_t | S_t=S]  \\&amp;=E[R_{t+1}+\gamma R_{t+2}+\gamma ^ 2R_{t+3}+\dots | S_t = S] \\&amp;=E[R_{t+1}+(\gamma R_{t+2}+\gamma ^ 2R_{t+3}+\dots) | S_t = S] \\&amp;=E[R_{t+1}+\gamma G_{t+1} | S_t = S] \\&amp;=E[R_{t+1}+\gamma V(S_t+1) | S_t = S]\end{align}\]</span> <span class="math inline">\(V(S)\)</span>取决于前一个状态的奖励 <spanclass="math inline">\(R_{t+1}\)</span>，也取决于当前状态的</p><p>Bellman Equation</p><h3id="马尔可夫决策过程-state-probability-reward-action">马尔可夫决策过程（State + Probability + Reward + Action）</h3><p>MDP 元组定义 <span class="math inline">\(\langleS,A,P,R,\gamma\rangle\)</span>：</p><p><span class="math inline">\(S\)</span> 有限状态集</p><p><span class="math inline">\(A\)</span> 有限动作集</p><p><span class="math inline">\(P\)</span> 状态转移概率矩阵， <spanclass="math inline">\(P_{ss&#39;}^a=P[S_{t+1}=s&#39; | S_t=s,A_t=a]\)</span></p><p><span class="math inline">\(R\)</span> 奖励函数， <spanclass="math inline">\(R_s^a=E[R_{t+1}|S_t=s, A_t=a]\)</span></p><p><span class="math inline">\(\gamma\)</span> 折扣函数， <spanclass="math inline">\(\gamma\in[0, 1]\)</span></p><p>目标：找到一条最佳路径，使得这条路径上的Reward最大</p><p>给定状态的动作概率分布（policy）：<spanclass="math inline">\(\pi\)</span> <span class="math display">\[\pi(a | s)=P[A_t=a|S_t=s]\]</span> 状态价值函数 <span class="math inline">\(v_{\pi}(s)\)</span><span class="math display">\[\begin{align}v_\pi(s) &amp;= E_\pi[G_t | S_t=s] \\&amp;=E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1}) | S_t=s]\end{align}\]</span> 从状态 <span class="math inline">\(s\)</span> 开始，遵循policy<span class="math inline">\(\pi\)</span>，期望获得的累计奖励。</p><p>动作价值函数 <span class="math inline">\(q_\pi(s,a)\)</span> <spanclass="math display">\[\begin{align}q_\pi(s,a)&amp;=E_\pi [G_t|S_t=s, A_t=a] \\&amp;=E_\pi [R_{t+1}+\gamma q_\pi(S_{t+1}, A_{t+1})|S_t=s, A_t=a] \\&amp;=R_s^a+\gamma \sum \limits_{s&#39;\in S}P_{ss&#39;}^av_\pi(s&#39;)\end{align}\]</span> 从状态 <span class="math inline">\(s\)</span> 开始，执行动作<span class="math inline">\(a\)</span>, 遵循policy <spanclass="math inline">\(\pi\)</span> 期望获得的累计奖励。</p><p>包含动作的状态转移过程：</p><p><img src="C:\Users\S\AppData\Roaming\Typora\typora-user-images\image-20231229150527704.png" alt="image-20231229150527704" style="zoom:50%;" /><img src="C:\Users\S\AppData\Roaming\Typora\typora-user-images\image-20231229150542970.png" alt="image-20231229150542970" style="zoom:50%;" /><img src="C:\Users\S\AppData\Roaming\Typora\typora-user-images\image-20231229150558157.png" alt="image-20231229150558157" style="zoom:50%;" /><img src="C:\Users\S\AppData\Roaming\Typora\typora-user-images\image-20231229160323899.png" alt="image-20231229160323899" style="zoom:50%;" /></p><p>贝尔曼期望方程求解 <span class="math inline">\(V^\pi\)</span> <spanclass="math display">\[v_\pi(S)=\sum \limits_{a\in A}\pi(a|s)q_\pi(s,a)\]</span></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;马尔可夫过程-state-probability&quot;&gt;马尔可夫过程 (State +
Probability)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://junlei-zhou.com/2023/12/08/hello-world/"/>
    <id>http://junlei-zhou.com/2023/12/08/hello-world/</id>
    <published>2023-12-08T12:50:07.908Z</published>
    <updated>2023-12-08T12:50:07.908Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very
first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; fo</summary>
      
    
    
    
    
  </entry>
  
</feed>
