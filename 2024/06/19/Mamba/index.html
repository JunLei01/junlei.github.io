<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Mamba | Welcome to Junlei's word!</title><meta name="author" content="坚竹韧周"><meta name="copyright" content="坚竹韧周"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Mamba  The Transformer architecture has been a major component in the success of Large Language Models (LLMs). It has been used for nearly all LLMs that are being used today, from open-source mode">
<meta property="og:type" content="article">
<meta property="og:title" content="Mamba">
<meta property="og:url" content="http://junlei-zhou.com/2024/06/19/Mamba/index.html">
<meta property="og:site_name" content="Welcome to Junlei&#39;s word!">
<meta property="og:description" content="Mamba  The Transformer architecture has been a major component in the success of Large Language Models (LLMs). It has been used for nearly all LLMs that are being used today, from open-source mode">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://junlei-zhou.com/img/avatar.jpg">
<meta property="article:published_time" content="2024-06-19T02:35:00.000Z">
<meta property="article:modified_time" content="2024-06-19T08:01:25.928Z">
<meta property="article:author" content="坚竹韧周">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Sequence Model">
<meta property="article:tag" content="State space model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://junlei-zhou.com/img/avatar.jpg"><link rel="shortcut icon" href="/img/URL_LOGO.png"><link rel="canonical" href="http://junlei-zhou.com/2024/06/19/Mamba/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Mamba',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-19 16:01:25'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="Welcome to Junlei's word!" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> Blog</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-blog"></i><span> Library</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Live</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fa fa-video"></i><span> Movie</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa fa-gamepad"></i><span> Game</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-paper-plane"></i><span> Comments</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Welcome to Junlei's word!"><span class="site-name">Welcome to Junlei's word!</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> Blog</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-blog"></i><span> Library</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Live</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fa fa-video"></i><span> Movie</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa fa-gamepad"></i><span> Game</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-paper-plane"></i><span> Comments</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Mamba</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-06-19T02:35:00.000Z" title="Created 2024-06-19 10:35:00">2024-06-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-06-19T08:01:25.928Z" title="Updated 2024-06-19 16:01:25">2024-06-19</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Mamba"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="mamba">Mamba</h1>
<!--本文是根据Mamba论文原作者撰写的Blog进行阅读的重点解读-->
<p>The Transformer architecture has been a major component in the
success of Large Language Models (LLMs). It has been used for nearly all
LLMs that are being used today, from open-source models like Mistral to
closed-source models like ChatGPT.</p>
<p>To further improve LLMs, new architectures are developed that might
even outperform the Transformer architecture. One of these methods is
<em>Mamba</em>, a <em>State Space Model</em>.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6bc1ca-a387-47ed-a9a6-077af838b359_1148x892.png" /></p>
<p>Mamba was proposed in the paper <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.00752">Mamba: Linear-Time Sequence
Modeling with Selective State Spaces</a>.<a
target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-1-141228095">1</a>
You can find its official implementation and model checkpoints in its <a
target="_blank" rel="noopener" href="https://github.com/state-spaces/mamba">repository</a>.</p>
<p>In this post, I will introduce the field of State Space Models in the
context of language modeling and explore concepts one by one to develop
an intuition about the field. Then, we will cover how Mamba might
challenge the Transformers architecture.</p>
<p>As a visual guide, expect many visualizations to develop an intuition
about Mamba and State Space Models!</p>
<h2 id="part-1-the-problem-with-transformers">Part 1: The Problem with
Transformers</h2>
<p>To illustrate why Mamba is such an interesting architecture, let’s do
a short re-cap of transformers first and explore one of its
disadvantages.</p>
<p>A Transformer sees any textual input as a <em>sequence</em> that
consists of <em>tokens</em>.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b8c299a-c0c0-46fe-86cf-b22e08a91b32_1776x544.png" /></p>
<p>A major benefit of Transformers is that whatever input it receives,
it can look back at any of the earlier tokens in the sequence to derive
its representation.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c01c75-1105-4aeb-a608-f00c85bbe5f7_1776x532.png" /></p>
<h3 id="the-core-components-of-transformers">The Core Components of
Transformers</h3>
<p>Remember that a Transformer consists of two structures, a set of
encoder blocks for representing text and a set of decoder blocks for
generating text. Together, these structures can be used for several
tasks, including translation.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62a21d60-2e84-4c19-a6fb-d2eff501af1c_1776x952.png" /></p>
<p>We can adopt this structure to create generative models by using only
decoders. This Transformer-based model, <em>Generative Pre-trained
Transformers</em> (GPT), uses decoder blocks to complete some input
text.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58e51959-d4ef-4fa9-a1c7-dab0e5ca4dc0_1776x1012.png" /></p>
<p>Let’s take a look at how that works!</p>
<h3 id="a-blessing-with-training">A Blessing with Training…</h3>
<p>A single decoder block consists of two main components, masked
self-attention followed by a feed-forward neural network.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5af9c5-5266-4c2b-b583-b20a19f19fcc_1776x464.png" /></p>
<p>Self-attention is a major reason why these models work so well. It
enables an uncompressed view of the entire sequence with fast
training.</p>
<p>So how does it work?</p>
<p>It creates a matrix comparing each token with every token that came
before. The weights in the matrix are determined by how relevant the
token pairs are to one another.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F167cfe80-2863-47c8-a969-cb2eeedbd353_1776x860.png" /></p>
<p>During training, this matrix is created in one go. The attention
between “<em>My</em>” and “<em>name</em>” does not need to be calculated
first before we calculate the attention between “<em>name</em>” and
“<em>is</em>”.</p>
<p>It enables <strong>parallelization</strong>, which speeds up training
tremendously!</p>
<h3 id="and-the-curse-with-inference">And the Curse with Inference!</h3>
<p>There is a flaw, however. When generating the next token, we need to
re-calculate the attention for the <em>entire sequence</em>, even if we
already generated some tokens.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb66f1965-fc44-4a61-b9c6-912c8120ecad_2420x580.png" /></p>
<p>Generating tokens for a sequence of length <em>L</em> needs roughly
<em>L²</em> computations which can be costly if the sequence length
increases.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F405074ed-aa8c-44b4-88a5-bae1dad0412e_2072x392.png" /></p>
<p>This need to recalculate the entire sequence is a major bottleneck of
the Transformer architecture.</p>
<p>Let’s look at how a “classic” technique, Recurrent Neural Networks,
solves this problem of slow inference.</p>
<hr />
<blockquote>
<p><strong>为什么Transformer的推理速度会很慢？</strong></p>
<p>因为Transformer在推理当前的状态时，需要计算包含之前所有状态的信息，也就是说，当计算到第N个状态时，需要同时计算包含前N个状态的信息以得出当前状态的取值，这也就时为什么以Transformer为基础的模型在输入时会有长度限制的前提条件。而相比较于Transformer，RNN模型有着比较快的推理速度因为它只用重点关注前面重点的状态，根据此来推理出当前状态，故而其计算速度快，但由于只关注之前的部分状态，也导致了其推理的准确度相比较于Transformer略有不及。</p>
</blockquote>
<h3 id="are-rnns-a-solution">Are RNNs a Solution?</h3>
<p>Recurrent Neural Networks (RNN) is a sequence-based network. It takes
two inputs at each time step in a sequence, namely the input at time
step <strong><em>t</em></strong> and a hidden state of the previous time
step <strong><em>t-1</em></strong>, to generate the next hidden state
and predict the output.</p>
<p>RNNs have a looping mechanism that allows them to pass information
from a previous step to the next. We can “unfold” this visualization to
make it more explicit.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddc71706-8da8-4c28-921b-675e9164c7ab_2404x872.png" /></p>
<p>When generating the output, the RNN only needs to consider the
previous hidden state and current input. It prevents recalculating all
previous hidden states which is what a Transformer would do.</p>
<p>In other words, RNNs can do inference fast as it scales linearly with
the sequence length! In theory, it can even have an <em>infinite context
length</em>.</p>
<p>To illustrate, let’s apply the RNN to the input text we have used
before.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2484541-f7b1-4950-b04f-5a3177596fbb_2228x808.png" /></p>
<p>Each hidden state is the aggregation of all previous hidden states
and is typically a compressed view.</p>
<p>There is a problem, however…</p>
<p>Notice that the last hidden state, when producing the name
“<em>Maarten</em>” does not contain information about the word
“<em>Hello</em>” anymore. RNNs tend to forget information over time
since they only consider one previous state.</p>
<p>This sequential nature of RNNs creates another problem. Training
cannot be done in parallel since it needs to go through each step at a
time sequentially.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819fcf9b-ea31-4954-8496-4b66c5b46dc2_2236x828.png" /></p>
<p>The problem with RNNs, compared to Transformers, is completely the
opposite! Its inference is incredibly fast but it is not
parallelizable.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7af6befe-e99d-4350-b555-21ce543cae53_2072x580.png" /></p>
<p>Can we somehow find an architecture that does parallelize training
like Transformers whilst still performing inference that scales linearly
with sequence length?</p>
<p>Yes! This is what Mamba offers but before diving into its
architecture, let’s explore the world of State Space Models first.</p>
<h2 id="part-2-the-state-space-model-ssm">Part 2: The <strong>State
Space Model (SSM)</strong></h2>
<p>A State Space Model (SSM), like the Transformer and RNN, processes
sequences of information, like text but also signals. In this section,
we will go through the basics of SSMs and how they relate to textual
data.</p>
<h3 id="what-is-a-state-space">What is a State Space?</h3>
<p>A State Space contains the minimum number of variables that fully
describe a system. It is a way to mathematically represent a problem by
defining a system's possible states.</p>
<p>Let’s simplify this a bit. Imagine we are navigating through a maze.
The “<em>state space</em>” is the map of all possible locations
(states). Each point represents a unique position in the maze with
specific details, like how far you are from the exit.</p>
<p>The “<em>state space representation</em>” is a simplified description
of this map. It shows where you are (current state), where you can go
next (possible future states), and what changes take you to the next
state (going right or left).</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6480800-2449-456a-87a7-27c8a4e9e718_2520x1388.png" /></p>
<p>Although State Space Models use equations and matrices to track this
behavior, it is simply a way to track where you are, where you can go,
and how you can get there.</p>
<p>The variables that describe a state, in our example the X and Y
coordinates, as well as the distance to the exit, can be represented as
“<em>state vectors</em>”.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c79eba-2559-4d9d-8999-bee33666f2e3_2364x736.png" /></p>
<p>Sounds familiar? That is because embeddings or vectors in language
models are also frequently used to describe the “state” of an input
sequence. For instance, a vector of your current position (state vector)
could look a bit like this:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ff8812a-64d2-4fc6-8e54-eb86222333b0_1496x444.png" /></p>
<p>In terms of neural networks, the “state” of a system is typically its
hidden state and in the context of Large Language Models, one of the
most important aspects of generating a new token.</p>
<p>What is a State Space Model?</p>
<p>SSMs are models used to describe these state representations and make
predictions of what their next state could be depending on some
input.</p>
<p>Traditionally, at time <strong><em>t</em></strong>, SSMs:</p>
<ul>
<li><p>map an input sequence <strong><em>x(t)</em></strong> — (e.g.,
moved left and down in the maze)</p></li>
<li><p>to a latent state representation <strong><em>h(t)</em></strong> —
(e.g., distance to exit and x/y coordinates)</p></li>
<li><p>and derive a predicted output sequence
<strong><em>y(t)</em></strong> — (e.g., move left again to reach the
exit sooner)</p></li>
</ul>
<p>However, instead of using <em>discrete</em> <em>sequences</em> (like
moving left once) it takes as input a <em>continuous</em>
<em>sequence</em> and predicts the output sequence.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5788c3e4-8794-4492-af87-3a45f7a6aa70_1992x624.png" /></p>
<p>SSMs assume that dynamic systems, such as an object moving in 3D
space, can be predicted from its state at time
<strong><em>t</em></strong> through two equations.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F32401c6d-39b6-4619-a75e-6b33d3268bca_2520x388.png" /></p>
<p>By solving these equations, we assume that we can uncover the
statistical principles to predict the state of a system based on
observed data (input sequence and previous state).</p>
<p>Its goal is to find this state representation
<strong><em>h(t)</em></strong> such that we can go from an input to an
output sequence.</p>
<p>[</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca5c7ae-3dbe-44d8-8b13-7f4dcc14a29b_2008x624.png" /></p>
<p>These two equations are the core of the State Space Model.</p>
<p>The two equations will be referenced throughout this guide. To make
them a bit more intuitive, they are <strong>color-coded</strong> so you
can quickly reference them.</p>
<p>The <strong>state equation</strong> describes how the state changes
(through <em>matrix A</em>) based on how the input influences the state
(through <em>matrix B</em>).</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0876819d-8a46-4187-9826-14391bfd47b9_1796x624.png" /></p>
<p>As we saw before, <strong><em>h(t)</em></strong> refers to our latent
state representation at any given time <strong><em>t</em></strong>, and
<strong><em>x(t)</em></strong> refers to some input.</p>
<p>The <strong>output equation</strong> describes how the state is
translated to the output (through <em>matrix C</em>) and how the input
influences the output (through <em>matrix D</em>).</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2e87708-9676-4a1c-b32c-d703026f64d9_1796x624.png" /></p>
<blockquote>
<p><strong>NOTE</strong>: Matrices <em>A</em>, <em>B</em>, <em>C</em>,
and <em>D</em> are also commonly refered to as <em>parameters</em> since
they are learnable.</p>
</blockquote>
<p>Visualizing these two equations gives us the following
architecture:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc358439e-c507-49f1-ac2e-5dedaccc2a8b_1728x364.png" /></p>
<p>Let’s go through the general technique step-by-step to understand how
these matrices influence the learning process.</p>
<p>Assume we have some input signal <strong><em>x(t)</em></strong>, this
signal first gets multiplied by <em>matrix B</em> which describes how
the inputs influence the system.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd6f8dae-2281-47af-8ba3-06bbdc594d1c_1956x360.png" /></p>
<p>The updated state (akin to the hidden state of a neural network) is a
latent space that contains the core “knowledge” of the environment. We
multiply the state with <em>matrix A</em> which describes how all the
internal states are connected as they represent the underlying dynamics
of the system.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cedc98a-d200-4fe4-b311-6d68dcaa50af_1956x572.png" /></p>
<p>As you might have noticed, <em>matrix A</em> is applied before
creating the state representations and is updated after the state
representation has been updated.</p>
<p>Then, we use <em>matrix C</em> to describe how the state can be
translated to an output.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8599487f-1023-4069-be7a-8056e63b0574_1956x572.png" /></p>
<p>Finally, we can make use of <em>matrix D</em> to provide a direct
signal from the input to the output. This is also often referred to as a
<em>skip-connection</em>.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf79721f-5cef-44da-98c5-f63a7839ebc3_1956x756.png" /></p>
<p>Since <em>matrix D</em> is similar to a skip-connection, the SSM is
often regarded as the following without the skip-connection.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca1d511-7d31-42a0-9220-2fb85b256efd_1956x864.png" /></p>
<p>Going back to our simplified perspective, we can now focus on
matrices <em>A</em>, <em>B</em>, and <em>C</em> as the core of the
SSM.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e52f4f0-d7ad-453d-a741-6dfa4a998964_1728x352.png" /></p>
<p>We can update the original equations (and add some pretty colors) to
signify the purpose of each matrix as we did before.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55df8ede-3a16-4473-8ea9-872fe199d3a1_1904x676.png" /></p>
<p>Together, these two equations aim to predict the state of a system
from observed data. Since the input is expected to be continuous, the
main representation of the SSM is a <strong>continuous-time
representation</strong>.</p>
<h3 id="from-a-continuous-to-a-discrete-signal">From a Continuous to a
Discrete Signal</h3>
<p>Finding the state representation <strong><em>h(t)</em></strong> is
analytically challenging if you have a continuous signal. Moreover,
since we generally have a discrete input (like a textual sequence), we
want to discretize the model.</p>
<p>To do so, we make use of the <em>Zero-order hold technique.</em> It
works as follows. First, every time we receive a discrete signal, we
hold its value until we receive a new discrete signal. This process
creates a continuous signal the SSM can use:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a2ffb18-2e66-4135-9888-98e9ab88d0d8_1488x472.png" /></p>
<p>How long we hold the value is represented by a new learnable
parameter, called the <em>step size</em> <strong>∆</strong>. It
represents the resolution of the input.</p>
<p>Now that we have a continuous signal for our input, we can generate a
continuous output and only sample the values according to the time steps
of the input.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F042ff699-81af-4479-b99f-92e4997c4c81_1488x476.png" /></p>
<p>These sampled values are our discretized output!</p>
<p>Mathematically, we can apply the Zero-order hold as follows:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6df4b59-6f76-4f13-a201-7b69e59df164_6200x1176.png" /></p>
<p>Together, they allow us to go from a continuous SSM to a discrete SSM
represented by a formulation that instead of a
<em>function-to-function</em>, <strong><em>x(t)</em></strong> →
<strong><em>y(t)</em></strong>, is now a <em>sequence-to-sequence,
<strong>x</strong></em><strong>ₖ</strong> →
<strong><em>y</em>ₖ</strong>:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc29cfbbb-ae41-4dc2-b899-9e0a81cba34d_1980x1012.png" /></p>
<p>Here, matrices <em>A</em> and <em>B</em> now represent discretized
parameters of the model.</p>
<p>We use <strong><em>k</em></strong> instead of
<strong><em>t</em></strong> to represent discretized timesteps and to
make it a bit more clear when we refer to a continuous versus a discrete
SSM.</p>
<blockquote>
<p><strong>NOTE:</strong> We are still saving the continuous form of
<em>Matrix A</em> and not the discretized version during training.
During training, the continuous representation is discretized.</p>
</blockquote>
<p>Now that we have a formulation of a discrete representation, let’s
explore how we can actually <em>compute</em> the model.</p>
<h3 id="the-recurrent-representation">The Recurrent Representation</h3>
<p>Our discretized SSM allows us to formulate the problem in specific
timesteps instead of continuous signals. A recurrent approach, as we saw
before with RNNs is quite useful here.</p>
<p>If we consider discrete timesteps instead of a continuous signal, we
can reformulate the problem with timesteps:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b70ba4-b068-44d1-8641-9b224d103c51_1980x548.png" /></p>
<p>At each timestep, we calculate how the current input
(<strong><em>Bx</em>ₖ</strong>) influences the previous state
(<strong>Ahₖ₋₁</strong>) and then calculate the predicted output
(<strong><em>Ch</em>ₖ</strong>).</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb4d0412-87fb-4507-bedb-4793588bd465_2116x788.png" /></p>
<p>This representation might already seem a bit familiar! We can
approach it the same way we did with the RNN as we saw before.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91ca51f7-9b9b-4f17-bccb-32a5a96f3339_2184x868.png" /></p>
<p>Which we can unfold (or unroll) as such:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1084e8a-a70d-450b-beb0-f18117ade5ed_2184x876.png" /></p>
<p>Notice how we can use this discretized version using the underlying
methodology of an RNN.</p>
<p>This technique gives us both the advantages and disadvantages of an
RNN, namely fast inference and slow training.</p>
<h3 id="the-convolution-representation">The Convolution
Representation</h3>
<p>Another representation that we can use for SSMs is that of
convolutions. Remember from classic image recognition tasks where we
applied filters (<em>kernels</em>) to derive aggregate features:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47f05950-bfad-4013-b854-679c9a47ada9_3216x2144.png" /></p>
<p>Since we are dealing with text and not images, we need a
1-dimensional perspective instead:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb943872f-de72-43e8-b2f1-cb8213f120a3_3216x1296.png" /></p>
<p>The kernel that we use to represent this “filter” is derived from the
SSM formulation:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05049821-2352-4c04-8fb2-07fe15c20a9c_2620x824.png" /></p>
<p>Let’s explore how this kernel works in practice. Like convolution, we
can use our SSM kernel to go over each set of tokens and calculate the
output:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9007d03b-c1c9-4b37-8c83-f27bfe8318f4_2620x1080.png" /></p>
<p>This also illustrates the effect padding might have on the output. I
changed the order of padding to improve the visualization but we often
apply it at the end of a sentence.</p>
<p>In the next step, the kernel is moved once over to perform the next
step in the calculation:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ed71fb-f237-4173-bb23-bd1bf02ff123_2620x1080.png" /></p>
<p>In the final step, we can see the full effect of the kernel:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4387b79-5e92-4fe2-8b30-2abf112f5a73_2620x1080.png" /></p>
<p>A major benefit of representing the SSM as a convolution is that it
can be trained in parallel like Convolutional Neural Networks (CNNs).
However, due to the fixed kernel size, their inference is not as fast
and unbounded as RNNs.</p>
<h3 id="the-three-representations">The Three Representations</h3>
<p>These three representations, <em>continuous</em>, <em>recurrent</em>,
and <em>convolutional</em> all have different sets of advantages and
disadvantages:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F682187d6-f402-44aa-8097-8a2e5b6179a7_2072x744.png" /></p>
<p>Interestingly, we now have efficient inference with the recurrent SSM
and parallelizable training with the convolutional SSM.</p>
<p>With these representations, there is a neat trick that we can use,
namely choose a representation depending on the task. During training,
we use the convolutional representation which can be parallelized and
during inference, we use the efficient recurrent representation:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c43c82d-9735-4d55-97bb-8ad6f504909e_1960x1008.png" /></p>
<p>This model is referred to as the <a
target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/05546b0e38ab9175cd905eebcc6ebb76-Abstract.html">Linear
State-Space Layer (LSSL)</a>.<a
target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-2-141228095">2</a></p>
<p>These representations share an important property, namely that of
<strong><em>Linear Time Invariance</em></strong> (LTI). LTI states that
the SSMs parameters, <em>A</em>, <em>B</em>, and <em>C</em>, are fixed
for all timesteps. This means that matrices <em>A</em>, <em>B</em>, and
<em>C</em> are the same for every token the SSM generates.</p>
<p>In other words, regardless of what sequence you give the SSM, the
values of <em>A</em>, <em>B</em>, and <em>C</em> remain the same. We
have a static representation that is not content-aware.</p>
<p>Before we explore how Mamba addresses this issue, let’s explore the
final piece of the puzzle, <em>matrix A</em>.</p>
<h3 id="the-importance-of-matrix-a">The Importance of Matrix
<em>A</em></h3>
<p>Arguably one of the most important aspects of the SSM formulation is
<em>matrix A</em>. As we saw before with the recurrent representation,
it captures information about the <em>previous</em> state to build the
<em>new</em> state.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07542fb1-d4b6-421e-8b2a-a3f0a7790939_2028x876.png" /></p>
<p>In essence, <em>matrix</em> <em>A</em> produces the hidden state:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47635355-6b9a-4981-af3a-7ee6a12b87b3_1412x468.png" /></p>
<p>Creating <em>matrix A</em> can therefore be the difference between
remembering only a few previous tokens and capturing every token we have
seen thus far. Especially in the context of the Recurrent representation
since it only <em>looks back</em> <em>at the previous state</em>.</p>
<p>So how can we create <em>matrix A</em> in a way that retains a large
memory (context size)?</p>
<p>We use Hungry Hungry Hippo! Or <a
target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2020/hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html">HiPPO</a><a
target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-3-141228095">3</a>
for <strong>Hi</strong>gh-order <strong>P</strong>olynomial
<strong>P</strong>rojection <strong>O</strong>perators. HiPPO attempts
to compress all input signals it has seen thus far into a vector of
coefficients.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07985a64-fc26-4ee8-9ec2-c488e4cb709a_1492x488.png" /></p>
<p>It uses <em>matrix A</em> to build a state representation that
captures recent tokens well and decays older tokens. Its formula can be
represented as follows:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bc7c768-7f0c-4983-a21e-70a4f587e6aa_2520x628.png" /></p>
<p>Assuming we have a square <em>matrix A</em>, this gives us:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8f5de9-6448-43d5-9878-c8cd1d938b7c_1436x708.png" /></p>
<p>Building <em>matrix A</em> using HiPPO was shown to be much better
than initializing it as a random matrix. As a result, it more accurately
reconstructs <em>newer</em> signals (recent tokens) compared to
<em>older</em> signals (initial tokens).</p>
<p>The idea behind the HiPPO Matrix is that it produces a hidden state
that memorizes its history.</p>
<p>Mathematically, it does so by tracking the coefficients of a <a
target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2019/hash/952285b9b7e7a1be5aa7849f32ffff05-Abstract.html">Legendre
polynomial</a> which allows it to approximate all of the previous
history.<a
target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-4-141228095">4</a></p>
<p>HiPPO was then applied to the recurrent and convolution
representations that we saw before to handle long-range dependencies.
The result was <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.00396">Structured
State Space for Sequences (S4)</a>, a class of SSMs that can efficiently
handle long sequences.<a
target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-5-141228095">5</a></p>
<p>It consists of three parts:</p>
<ul>
<li><p>State Space Models</p></li>
<li><p>HiPPO for handling <strong>long-range
dependencies</strong></p></li>
<li><p>Discretization for creating <strong>recurrent</strong> and
<strong>convolution</strong> representations</p></li>
</ul>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb055ec5-f8c7-4862-ab88-f4fb38abf042_1892x844.png" /></p>
<p>This class of SSMs has several benefits depending on the
representation you choose (recurrent vs. convolution). It can also
handle long sequences of text and store memory efficiently by building
upon the HiPPO matrix.</p>
<blockquote>
<p><strong>NOTE</strong>: If you want to dive into more of the technical
details on how to calculate the HiPPO matrix and build a S4 model
yourself, I would HIGHLY advise going through the <a
target="_blank" rel="noopener" href="https://srush.github.io/annotated-s4/">Annotated S4</a>.</p>
</blockquote>
<h2 id="part-3-mamba---a-selective-ssm">Part 3: Mamba - A Selective
SSM</h2>
<p>We finally have covered all the fundamentals necessary to understand
what makes Mamba special. State Space Models can be used to model
textual sequences but still have a set of disadvantages we want to
prevent.</p>
<p>In this section, we will go through Mamba’s two main
contributions:</p>
<ol type="1">
<li><p>A <strong>selective scan algorithm</strong>, which allows the
model to filter (ir)relevant information</p></li>
<li><p>A <strong>hardware-aware algorithm</strong> that allows for
efficient storage of (intermediate) results through <em>parallel
scan</em>, <em>kernel fusion</em>, and <em>recomputation</em>.</p></li>
</ol>
<p>Together they create the <em>selective SSM</em> or <em>S6</em> models
which can be used, like self-attention, to create <em>Mamba
blocks</em>.</p>
<p>Before exploring the two main contributions, let’s first explore why
they are necessary.</p>
<h3 id="what-problem-does-it-attempt-to-solve">What Problem does it
attempt to Solve?</h3>
<p>State Space Models, and even the S4 (Structured State Space Model),
perform poorly on certain tasks that are vital in language modeling and
generation, namely <em>the ability to focus on or ignore particular
inputs</em>.</p>
<p>We can illustrate this with two synthetic tasks, namely
<strong>selective copying</strong> and <strong>induction
heads</strong>.</p>
<p>In the <strong>selective copying</strong> task, the goal of the SSM
is to copy parts of the input and output them in order:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3f1d1fb-7603-4f86-a2e4-88e0496a1f08_2120x464.png" /></p>
<p>However, a (recurrent/convolutional) SSM performs poorly in this task
since it is <strong><em>Linear Time Invariant</em>.</strong> As we saw
before, the matrices <em>A</em>, <em>B</em>, and <em>C</em> are the same
for every token the SSM generates.</p>
<p>As a result, an SSM cannot perform <em>content-aware reasoning</em>
since it treats each token equally as a result of the fixed A, B, and C
matrices. This is a problem as we want the SSM to reason about the input
(prompt).</p>
<p>The second task an SSM performs poorly on is <strong>induction
heads</strong> where the goal is to reproduce patterns found in the
input:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27775742-18d2-4f2c-b792-5706976e75e3_2120x744.png" /></p>
<p>In the above example, we are essentially performing one-shot
prompting where we attempt to “teach” the model to provide an
“<strong><em>A:</em></strong>” response after every
“<strong><em>Q:</em></strong>”. However, since SSMs are time-invariant
it cannot select which previous tokens to recall from its history.</p>
<p>Let’s illustrate this by focusing on <em>matrix B</em>. Regardless of
what the input <strong><em>x</em></strong> is, <em>matrix B</em> remains
exactly the same and is therefore independent of
<strong><em>x</em></strong>:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ee2bd7a-b99b-4871-8396-69cb7dd13cf5_1480x808.png" /></p>
<p>Likewise, <em>A</em> and <em>C</em> also remain fixed regardless of
the input. This demonstrates the <em>static</em> nature of the SSMs we
have seen thus far.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fea51ca-8458-4216-bf1c-c880a23504b4_1412x484.png" /></p>
<p>In comparison, these tasks are relatively easy for Transformers since
they <em>dynamically</em> change their attention based on the input
sequence. They can selectively “look” or “attend” at different parts of
the sequence.</p>
<p>The poor performance of SSMs on these tasks illustrates the
underlying problem with time-invariant SSMs, the static nature of
matrices <em>A</em>, <em>B</em>, and <em>C</em> results in problems with
<em>content-awareness</em>.</p>
<h3 id="selectively-retain-information">Selectively Retain
Information</h3>
<p>The recurrent representation of an SSM creates a small state that is
quite efficient as it compresses the entire history. However, compared
to a Transformer model which does no compression of the history (through
the attention matrix), it is much less powerful.</p>
<p>Mamba aims to have the best of both worlds. A small state that is as
powerful as the state of a Transformer:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84b8a71a-6310-416b-8622-e9166593171e_1540x464.png" /></p>
<p>As teased above, it does so by compressing data selectively into the
state. When you have an input sentence, there is often information, like
stop words, that does not have much meaning.</p>
<p>To selectively compress information, we need the parameters to be
dependent on the input. To do so, let’s first explore the dimensions of
the input and output in an SSM during training:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9376222e-b232-4458-a72c-bd741d8a031c_1436x500.png" /></p>
<p>In a Structured State Space Model (S4), the matrices <em>A</em>,
<em>B</em>, and <em>C</em> are independent of the input since their
dimensions <strong><em>N</em></strong> and <strong><em>D</em></strong>
are static and do not change.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e93b701-df74-4954-be5c-c0d83779d3df_1412x532.png" /></p>
<p>Instead, Mamba makes matrices <em>B</em> and <em>C,</em> and even the
<em>step size</em> <strong>∆</strong><em>,</em> dependent on the input
by incorporating the sequence length and batch size of the input:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdccefffd-5712-45ab-9821-c794bce65d7d_1412x596.png" /></p>
<p>This means that for every input token, we now have different
<em>B</em> and <em>C</em> matrices which solves the problem with
content-awareness!</p>
<blockquote>
<p><strong>NOTE</strong>: Matrix <em>A</em> remains the same since we
want the state itself to remain static but the way it is influenced
(through <em>B</em> and <em>C</em>) to be dynamic.</p>
</blockquote>
<p>Together, they <em>selectively</em> choose what to keep in the hidden
state and what to ignore since they are now dependent on the input.</p>
<p>A smaller <em>step size</em> <strong>∆</strong> results in ignoring
specific words and instead using the previous context more whilst a
larger <em>step size</em> <strong>∆</strong> focuses on the input words
more than the context:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06b21aab-aa32-450a-ae02-b976a2c9f9d8_2520x616.png" /></p>
<h3 id="the-scan-operation">The Scan Operation</h3>
<p>Since these matrices are now <em>dynamic</em>, they cannot be
calculated using the convolution representation since it assumes a
<em>fixed</em> kernel. We can only use the recurrent representation and
lose the parallelization the convolution provides.</p>
<p>To enable parallelization, let’s explore how we compute the output
with recurrency:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed3ad33b-7b7f-4b69-b28e-8437c7b71e2e_1540x536.png" /></p>
<p>Each state is the sum of the previous state (multiplied by
<em>A</em>) plus the current input (multiplied by <em>B</em>). This is
called a <em>scan operation</em> and can easily be calculated with a for
loop.</p>
<p>Parallelization, in contrast, seems impossible since each state can
only be calculated if we have the previous state. Mamba, however, makes
this possible through the <em><a
target="_blank" rel="noopener" href="https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda">parallel
scan</a></em> algorithm.</p>
<p>It assumes the order in which we do operations does not matter
through the associate property. As a result, we can calculate the
sequences in parts and iteratively combine them:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F191fdabe-6b38-4e6b-a9f0-8240feef0a9d_1640x1100.png" /></p>
<p>Together, dynamic matrices <em>B</em> and <em>C</em>, and the
parallel scan algorithm create the <strong><em>selective scan
algorithm</em></strong> to represent the dynamic and fast nature of
using the recurrent representation.</p>
<h3 id="hardware-aware-algorithm">Hardware-aware Algorithm</h3>
<p>A disadvantage of recent GPUs is their limited transfer (IO) speed
between their small but highly efficient SRAM and their large but
slightly less efficient DRAM. Frequently copying information between
SRAM and DRAM becomes a bottleneck.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a1d4fa3-526d-488e-8768-c7208f018eb4_1728x300.png" /></p>
<p>Mamba, like Flash Attention, attempts to limit the number of times we
need to go from DRAM to SRAM and vice versa. It does so through
<em>kernel fusion</em> which allows the model to prevent writing
intermediate results and continuously performing computations until it
is done.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F563b3792-701a-42b1-9b68-7b6457f5e63d_1728x344.png" /></p>
<p>We can view the specific instances of DRAM and SRAM allocation by
visualizing Mamba’s base architecture:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F724eceb1-4356-4ac5-b44e-f7fabce3b472_1728x580.png" /></p>
<p>Here, the following are fused into one kernel:</p>
<ul>
<li><p>Discretization step with <em>step size</em>
<strong>∆</strong></p></li>
<li><p>Selective scan algorithm</p></li>
<li><p>Multiplication with <em>C</em></p></li>
</ul>
<p>The last piece of the hardware-aware algorithm is
<em>recomputation</em>.</p>
<p>The intermediate states are not saved but are necessary for the
backward pass to compute the gradients. Instead, the authors recompute
those intermediate states <em>during</em> the backward pass.</p>
<p>Although this might seem inefficient, it is much less costly than
reading all those intermediate states from the relatively slow DRAM.</p>
<p>We have now covered all components of its architecture which is
depicted using the following image from its article:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc840fb8-2e24-4103-95c8-afa306ce0cfc_2409x743.png" /></p>
<p><strong>The Selective SSM.</strong> Retrieved from: Gu, Albert, and
Tri Dao. "Mamba: Linear-time sequence modeling with selective state
spaces." <em>arXiv preprint arXiv:2312.00752</em> (2023).</p>
<p>This architecture is often referred to as a <strong><em>selective
SSM</em></strong> or <strong><em>S6</em></strong> model since it is
essentially an S4 model computed with the selective scan algorithm.</p>
<h3 id="the-mamba-block">The Mamba Block</h3>
<p>The <em>selective SSM</em> that we have explored thus far can be
implemented as a block, the same way we can represent self-attention in
a decoder block.</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb76db77-1dba-42bd-8de4-4bce240ff67e_1776x1012.png" /></p>
<p>Like the decoder, we can stack multiple Mamba blocks and use their
output as the input for the next Mamba block:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc94d349d-8620-45a9-8095-7c27de8b7865_1660x1356.png" /></p>
<p>It starts with a linear projection to expand upon the input
embeddings. Then, a convolution before the <em>Selective SSM</em> is
applied to prevent independent token calculations.</p>
<p>The <em>Selective SSM</em> has the following properties:</p>
<ul>
<li><p><em>Recurrent SSM</em> created through
<em>discretization</em></p></li>
<li><p><em>HiPPO</em> initialization on matrix <em>A</em> to capture
<em>long-range dependencies</em></p></li>
<li><p>S<em>elective scan algorithm</em> to selectively compress
information</p></li>
<li><p><em>Hardware-aware algorithm</em> to speed up
computation</p></li>
</ul>
<p>We can expand on this architecture a bit more when looking at the
code implementation and explore how an end-to-end example would look
like:</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa67d7341-9a43-4c67-aa88-6e802c0902ae_1660x2040.png" /></p>
<p>Notice some changes, like the inclusion of normalization layers and
softmax for choosing the output token.</p>
<p>When we put everything together, we get both fast inference and
training and even unbounded context!</p>
<p><img
src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe528e5fa-0dd8-4e6c-b31a-5248aaee6c68_2072x912.png" /></p>
<p>Using this architecture, the authors found it matches and sometimes
even exceeds the performance of Transformer models of the same size!</p>
<h2 id="conclusion"><strong>Conclusion</strong></h2>
<p>This concludes our journey in State Space Models and the incredible
Mamba architecture using a selective State Space Model. Hopefully, this
post gives you a better understanding of the potential of State Space
Models, particularly Mamba. Who knows if this is going to replace the
Transformers but for now, it is incredible to see such different
architectures getting well-deserved attention!</p>
<h2 id="resources">Resources</h2>
<p>Hopefully, this was an accessible introduction to Mamba and State
Space Models. If you want to go deeper, I would suggest the following
resources:</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://srush.github.io/annotated-s4/">The Annotated
S4</a> is a JAX implementation and guide through the S4 model and is
highly advised!</p></li>
<li><p>A great <a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ouF-H35atOY">YouTube video</a>
introducing Mamba by building it up through foundational
papers.</p></li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/state-spaces/mamba">The Mamba
repository</a> with <a
target="_blank" rel="noopener" href="https://huggingface.co/state-spaces">checkpoints on Hugging
Face</a>.</p></li>
<li><p>An amazing series of blog posts (<a
target="_blank" rel="noopener" href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-1">1</a>, <a
target="_blank" rel="noopener" href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-2">2</a>, <a
target="_blank" rel="noopener" href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-3">3</a>)
that introduces the S4 model.</p></li>
<li><p>The <a
target="_blank" rel="noopener" href="https://jameschen.io/jekyll/update/2024/02/12/mamba.html">Mamba
No. 5 (A Little Bit Of...)</a> blog post is a great next step to dive
into more technical details about Mamba but still from an amazingly
intuitive perspective.</p></li>
<li><p>And of course, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.00752">the
Mamba paper</a>! It was even used for DNA modeling and speech
generation.</p></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://junlei-zhou.com">坚竹韧周</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://junlei-zhou.com/2024/06/19/Mamba/">http://junlei-zhou.com/2024/06/19/Mamba/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a><a class="post-meta__tags" href="/tags/Sequence-Model/">Sequence Model</a><a class="post-meta__tags" href="/tags/State-space-model/">State space model</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq,email,copy_link"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/05/29/Support-Vector-Machine/" title="Support-Vector-Machine"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">Support-Vector-Machine</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">坚竹韧周</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/JunLei01"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/JunLei01" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:zjunlei1225@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#mamba"><span class="toc-number">1.</span> <span class="toc-text">Mamba</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#part-1-the-problem-with-transformers"><span class="toc-number">1.1.</span> <span class="toc-text">Part 1: The Problem with
Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#the-core-components-of-transformers"><span class="toc-number">1.1.1.</span> <span class="toc-text">The Core Components of
Transformers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-blessing-with-training"><span class="toc-number">1.1.2.</span> <span class="toc-text">A Blessing with Training…</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#and-the-curse-with-inference"><span class="toc-number">1.1.3.</span> <span class="toc-text">And the Curse with Inference!</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#are-rnns-a-solution"><span class="toc-number">1.1.4.</span> <span class="toc-text">Are RNNs a Solution?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#part-2-the-state-space-model-ssm"><span class="toc-number">1.2.</span> <span class="toc-text">Part 2: The State
Space Model (SSM)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#what-is-a-state-space"><span class="toc-number">1.2.1.</span> <span class="toc-text">What is a State Space?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#from-a-continuous-to-a-discrete-signal"><span class="toc-number">1.2.2.</span> <span class="toc-text">From a Continuous to a
Discrete Signal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-recurrent-representation"><span class="toc-number">1.2.3.</span> <span class="toc-text">The Recurrent Representation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-convolution-representation"><span class="toc-number">1.2.4.</span> <span class="toc-text">The Convolution
Representation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-three-representations"><span class="toc-number">1.2.5.</span> <span class="toc-text">The Three Representations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-importance-of-matrix-a"><span class="toc-number">1.2.6.</span> <span class="toc-text">The Importance of Matrix
A</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#part-3-mamba---a-selective-ssm"><span class="toc-number">1.3.</span> <span class="toc-text">Part 3: Mamba - A Selective
SSM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#what-problem-does-it-attempt-to-solve"><span class="toc-number">1.3.1.</span> <span class="toc-text">What Problem does it
attempt to Solve?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#selectively-retain-information"><span class="toc-number">1.3.2.</span> <span class="toc-text">Selectively Retain
Information</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-scan-operation"><span class="toc-number">1.3.3.</span> <span class="toc-text">The Scan Operation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hardware-aware-algorithm"><span class="toc-number">1.3.4.</span> <span class="toc-text">Hardware-aware Algorithm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-mamba-block"><span class="toc-number">1.3.5.</span> <span class="toc-text">The Mamba Block</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conclusion"><span class="toc-number">1.4.</span> <span class="toc-text">Conclusion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#resources"><span class="toc-number">1.5.</span> <span class="toc-text">Resources</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/19/Mamba/" title="Mamba">Mamba</a><time datetime="2024-06-19T02:35:00.000Z" title="Created 2024-06-19 10:35:00">2024-06-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/05/29/Support-Vector-Machine/" title="Support-Vector-Machine">Support-Vector-Machine</a><time datetime="2024-05-29T07:56:16.000Z" title="Created 2024-05-29 15:56:16">2024-05-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/27/markov-process/" title="markov_process">markov_process</a><time datetime="2023-12-27T09:33:42.000Z" title="Created 2023-12-27 17:33:42">2023-12-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/08/hello-world/" title="Hello World">Hello World</a><time datetime="2023-12-08T12:50:07.908Z" title="Created 2023-12-08 20:50:07">2023-12-08</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 坚竹韧周</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>