<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Support-Vector-Machine</title>
      <link href="/2024/05/29/Support-Vector-Machine/"/>
      <url>/2024/05/29/Support-Vector-Machine/</url>
      
        <content type="html"><![CDATA[<h1 id="支持向量机-Support-Vector-Machine"><a href="#支持向量机-Support-Vector-Machine" class="headerlink" title="支持向量机 (Support Vector Machine)"></a>支持向量机 (Support Vector Machine)</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>支持向量机是无监督机器学习中一个经典的算法，具有完善的数学理论和推理证明。本文将从SVM问题定义，模型建立，数学推理对SVM进行详细的论述。</p><h2 id="线性可分-Linear-S"><a href="#线性可分-Linear-S" class="headerlink" title="线性可分 (Linear S)"></a>线性可分 (Linear S)</h2><h4 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h4><p>在一个样本空间中，给定训练样本集，在样本空间中能够找到一个划分超平面，将不同类别的样本区分开。</p><p><img src="https://imgur.la/images/2024/06/03/image-20240530093244843.png" alt="image 20240530093244843" border="0" style="zoom:50%;" ></p><center><p>图1：线性可分，存在多个超平面将两类训练样本分开</p></center><p><strong>结论1：在一个空间中，如果存在一条直线能够划分两个点集，那么将存在无数条直线能够划分这两个点集。(证明见附录)</strong></p><h4 id="数学定义"><a href="#数学定义" class="headerlink" title="数学定义"></a><strong>数学定义</strong></h4><p>$D_1$和 $D_2$ 是 n 维欧氏空间中的两个点集。如果存在 n 维向量 $w$ 和实数 $b$ 使得所有属于 $D_0$ 的点 $x_i$ 都有 $wx_i+b&gt;0$, 而对于所有属于 $D_1$ 的点 $x_j$ 则有 $wx_j+b&lt;0$，则我们称 $D_0$ 和 $D_1$ 线性可分。</p><h2 id="SVM解决线性问题"><a href="#SVM解决线性问题" class="headerlink" title="SVM解决线性问题"></a>SVM解决线性问题</h2><h4 id="线性问题定义"><a href="#线性问题定义" class="headerlink" title="线性问题定义"></a>线性问题定义</h4><p>由 <strong>结论1</strong> 可得，对于一个线性可分的问题，存在无数条满足要求的直线，那么究竟哪一条直线是最好的？</p><p><img src="https://imgur.la/images/2024/06/03/image-20240530104549138.png" alt="image-20240530104549138" style="zoom:50%;" /></p><center><p>图2：线性可分，存在多个超平面将两类训练样本分开</p></center><p>直观上看，应该去找位于两类训练样本“正中间”的划分超平面，即图中红色的直线，因为该划分超平面对训练样本局部扰动的“容忍”性最好，例如，由于训练集的局限性或噪声的因素，训练集外的样本可能比图中的训练样本更接近两个类的分隔界，这将使许多划分超平面出现错误，而红色的超平面受影响最小，换言之，这个划分超平面所产生的分类结果是最鲁棒的，对未见示例的泛化能力最强。</p><p><strong>问题1：我们怎样定义这样一个最好的直线？</strong>  答：为每条直线定义一个性能指标，将每条直线平移直至该直线能够插到最边缘的样本点，如下图所示：</p><p><img src="https://imgur.la/images/2024/06/03/image-20240530112035809.png" alt="image-20240530112035809" style="zoom:50%;" /></p><center><p>图3：支持向量与间隔</p></center><p>如图3所示，我们做如下定义：</p><p>$r$ : 间隔(Margin)，将平行线平移直至插到支持向量的距离</p><p>$x$ : 训练集中的样本</p><p>$y$ : 训练集中样本对应的标签</p><p>$D={(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n) },y_i\in{-1,+1}$ : 训练样本集</p><p>$w^Tx+b=0$ : 超平面(Hypeplane)，即上文中提到的直线，在二维空间中是一条直线，在高维空间中，则是超平面 (hyperplane)。</p><h4 id="SVM基本型"><a href="#SVM基本型" class="headerlink" title="SVM基本型"></a>SVM基本型</h4><p>支持向量机——线性可分，本质为最大化 <strong>Margin</strong> 问题，属于凸优化中的 <strong>二次规划</strong> 问题。</p><ul><li>最小化  (Minimize) :             $\frac{1}{2}||w||^2$</li><li>限制条件 (Subject to) :         $y_i[w^Tx_i+b]\geq1, \ \ \ \ \ \ \ \ \ \ \ \ i=1，2,\dots,n$</li></ul><p><strong>解释：</strong></p><p><strong>定理1：</strong>$w^T+b=0$ 与 $aw^T+ab=0$ 是同一个超平面，若 $(w,b)$ 满足 $y_i[w^Tx_i+b]\geq1$ 那么 $(aw,ab)$ 也满足 $y_i[aw^Tx_i+ab]\geq1$ 。</p><p><strong>定理2：</strong>点 $(x_0,y_0)$ 到直线 $ax+by+c=0$ 的距离 $d=\frac{|ax_0+by_0+c|}{\sqrt{a^2+b^2}}$ ; 点到超平面 $H$ 的距离即 $r=\frac{|w^Tx_0+b|}{||w||}$.  </p><p>由 <strong>定理1</strong>我们可以通过缩放将 $|w^Tx_0+b|$ 通过乘以一个缩放因子 $a$ 使得 $|aw^Tx_0+ab|=1$, 则此时 $r=\frac{1}{||w||}$, 故而最大化 <strong>Margin</strong> 即可以转变为最小化 $||w||$ 问题。由于我们在计算机中使用梯度下降进行最优化求解，为了求导数方便，因此常常使用$||w||^2$ 或者 $\frac{1}{2}||w||^2$ 的形式来代替 $||w||$ 。</p><p>限制条件 : 即对于所有的样本点，除了支持向量，其他的点都必须比支持向量到超平面的距离要远。</p><p>到此，SVM的基本型便定义完成，这是一个简单的凸优化问题，我们可以使用简单的凸优化求解方法来实现计算 (如梯度下降法)。</p><h2 id="SVM解决非线性问题"><a href="#SVM解决非线性问题" class="headerlink" title="SVM解决非线性问题"></a>SVM解决非线性问题</h2><h4 id="非线性可分"><a href="#非线性可分" class="headerlink" title="非线性可分"></a>非线性可分</h4><p><img src="https://imgur.la/images/2024/06/03/SVM_image4.png" alt="SVM image4" border="0" style="zoom:40%;" ><img src="https://imgur.la/images/2024/06/03/SVM_image5.png" alt="SVM image5" border="0" style="zoom:40%;" ></p><center><p>图4：非线性可分，在二维空间中，我们无法找到一条直线来分离这两个类别</p></center><p>如图4所示，对于非线性可分的情况，在二维空间中我们无法使用一条直线或者一个平面将两个类别完全区分。在低维空间中我们无法有效地将这两类点完全区分，但在高维空间中，我们可以找到一个超平面来完全区分这两类点 (<a href="https://en.wikipedia.org/wiki/Cover%27s_theorem"><strong>Cover’s theorem</strong></a>) .</p><p><img src="https://imgur.la/images/2024/06/03/SVM_image64b9b723ba732350b.png" alt="SVM image6" border="0" style="zoom: 25%;" ></p><center><p>图5：在低维空间中非线性可分，但将这些点映射到高维空间中则线性可分</p></center><p>因此，为了寻找一个超平面，我们需要将原本的样本点 $x$ 映射到一个高维空间中去，即设 $\varphi(x)$ 为映射函数，$\varphi(x): \mathcal{X}\longrightarrow \mathcal{H}$，其中 $\mathcal{X}$ 为原输入空间，$\mathcal{H}$ 为特征空间( $\mathcal{H}$ 是Hibert Space， 即完备的内积空间)</p><script type="math/tex; mode=display">x \stackrel{\varphi}{\longrightarrow}\varphi(x)</script><p><strong>问题2:  如何寻找这个高维映射方法？</strong>           直接将 $x$ 映射为为无限维。</p><p>但此时由于 $\varphi(x)$ 是无限维，无法计算。 此时我们使用 <strong>核技巧(kernel trick) : 在输入空间中找到一个核函数(kernel function) $K(x_1,x_2)$ 使得 $K(x_1,x_2)=&lt;\varphi(x_1),\varphi(x_2)&gt;$, 从而可以直接在低维空间中计算出结果，加速核方法计算。</strong> (相关性质及定义请见 <strong>附录：核函数</strong>)</p><p><strong>问题3：如何实现非线性中完全区分不同类的点？</strong></p><p>为了使线性可分的SVM优化泛化到非线性可分的情况，我们允许支持向量机在一些样本上出错，为此引入了 <strong>软间隔 (Soft Margin)</strong>，允许少部分样本可以不满足约束 $y_i[w^Tx_i+b]\geq1$, 但在最大化间隔的同时，应该保证不满足约束的样本数量尽可能的少，因此优化目标进一步可以变为：</p><script type="math/tex; mode=display">\mathop{\min}_{w,b}\  \frac{1}{2}||\omega||^2+C\sum \limits_{i=1}^{N}\mathscr{l}_{0/1}(y_i[w^Tx_i+b]-1)</script><p>其中 $C&gt;0$ 是一个常数，$\mathscr{l}_{0/1}$ 是”0/1损失函数”</p><script type="math/tex; mode=display">\mathscr{l}_{0/1}(z)= \left \{ \begin{array}{rcl} 1, & \mbox{if}\ \ z<0; \\ 0, & \mbox{otherwise.} \end{array} \right.</script><p>其中，当 $C$ 为无穷大时，新的优化目标式子能够迫使所有的样本均满足约束式子。此时 $\frac{1}{2}||\omega||^2+C\sum \limits<em>{i=1}^{N}\mathscr{l}</em>{0/1}(y_i[w^Tx_i+b]-1)$ 与 $\frac{1}{2}||w||^2$ 等价；当 $C$ 为有限值时，新的优化目标允许一部分样本不满足约束。</p><p>然而，$\mathscr{l}<em>{0/1}$ 非凸、非连续，数学性质不好，导致不易直接求解，故而我们通常使用其他的一些函数代替 $\mathscr{l}</em>{0/1}$ 并称之为 “替代损失 (surrogate loss)”。这些替代损失通常具有较好的数学性质，通常是凸的连续函数且是 $\mathscr{l}<em>{0/1}$ 的上界。如我们采用hinge损失代替 $\mathscr{l}</em>{0/1}$ 则优化目标可以变为</p><script type="math/tex; mode=display">\mathop{\min}_{w,b}\  \frac{1}{2}||\omega||^2+C\sum \limits_{i=1}^{N}\max(0,\ 1-y_i[w^Tx_i+b])</script><p>实际中我们可以引入<strong>松弛变量 (Slack Variable) $\xi_i\geq0$</strong> 来代替上式中的替代损失，得到</p><script type="math/tex; mode=display">\mathop{\min}_{w,b}\ \frac{1}{2}||\omega||^2+C\sum \limits_{i=1}^{N}\xi_i</script><p>从机器学习的角度来看，非线性SVM的优化目标可以看作为对线性的SVM优化目标添加了一个 <strong>正则项 (Regulation) $C\sum \limits_{i=1}^{N}\xi_i$</strong> </p><h4 id="非线性SVM优化"><a href="#非线性SVM优化" class="headerlink" title="非线性SVM优化"></a>非线性SVM优化</h4><p>支持向量机——非线性可分 (“软间隔”支持向量机)</p><ul><li>最小化 (Minimize) :             $\frac{1}{2}||w||^2+C\sum \limits_{i=1}^{N}\xi_i$</li><li>限制条件 (Subject to) :         <ul><li>​    $y_i[w^T\varphi(x_i)+b]\geq1-\xi_i, \ \ \ \ \ \ \ \ \ \ \ \ i=1，2,\dots,n$</li><li>​    $\xi_i\geq0$</li></ul></li></ul><p>在这个式子中存在高维映射 $\varphi(x_i)$ ,因而无法直接求解，因此我们要将该优化问题转换为可以求解的形式。</p><h4 id="解非线性SVM的优化问题"><a href="#解非线性SVM的优化问题" class="headerlink" title="解非线性SVM的优化问题"></a>解非线性SVM的优化问题</h4><p><strong>关键：将非线性的SVM优化问题转换为其对应的对偶问题，利用求其对偶问题的解来代替求原非线性SVM问题的解</strong> </p><p>(原问题与对偶问题的关系证明见附录——优化理论：原问题与对偶问题)</p><ul><li><p>[ ] <strong>Step 1:</strong> 为了方便转换为其对应的对偶问题，首先要转变原问题的形式，使其与附录中的原问题的形式相一致。</p><p><img src="https://imgur.la/images/2024/06/05/SVM_image7.png" alt="SVM image7" border="0"></p><center><p>图6：将非线性SVM的优化问题形式转换，与原问题中的形式保持一致</p></center><p><strong>解释：</strong>其中 $\xi<em>i$ 是松弛变量，我们调整其取值范围为 $\xi_i\leq0$ ,相比较于原式子，只是在原来的 $\xi_i$ 前面加了一个 “负号”。因此保证之前的优化目标不变，故而要变为 $\frac{1}{2}||w||^2-C\sum \limits</em>{i=1}^{N}\xi<em>i$ ,限制条件变为 $y_i[w^T\varphi(x_i)+b]\geq1+\xi_i$ , 将其移项得 $1+\xi_i-y_iw^T\varphi(x_i)-y_ib\leq0$ 。此时原问题中的 $f(w)\Longrightarrow \frac{1}{2}||\omega||^2-C\sum \limits</em>{i=1}^{N}\xi_i$ , $g_i(w)\leq0 \Longrightarrow 1+\xi_i-y_iw^T\varphi(x_i)-y_ib\leq0 和 \xi_i\leq0$ 。</p><h6 id="非线性SVM优化-转换后"><a href="#非线性SVM优化-转换后" class="headerlink" title="非线性SVM优化 (转换后)"></a>非线性SVM优化 (转换后)</h6><p>支持向量机——非线性可分 (“软间隔”支持向量机)</p><ul><li>最小化 (Minimize) :             $\frac{1}{2}||\omega||^2-C\sum \limits_{i=1}^{N}\xi_i$</li><li>限制条件 (Subject to) :         <ul><li>​    $1+\xi_i-y_iw^T\varphi(x_i)-y_ib\leq0, \ \ \ \ \ \ \ \ \ \ \ \ i=1，2,\dots,n$</li><li>​    $\xi_i\leq0$</li></ul></li></ul></li><li><p>[ ] <strong>Step 2: </strong> 寻找转换后的非线性SVM优化问题的对偶问题，根据<strong>附录——优化理论：原问题与对偶问题</strong>我们可得其对偶问题为：</p><h6 id="非线性SVM优化问题的对偶问题"><a href="#非线性SVM优化问题的对偶问题" class="headerlink" title="非线性SVM优化问题的对偶问题"></a>非线性SVM优化问题的对偶问题</h6><ul><li>最大化 (Maximum) :         $\theta(\alpha,\beta)=\inf \limits<em>{w,\xi_i,b}{\frac{1}{2}||\omega||^2-C \sum\limits</em>{i=1}^N\xi<em>i+\sum\limits</em>{i=1}^{N}\alpha<em>i[1+\xi_i-y_iw^T\varphi(x_i)-y_ib]+\sum\limits</em>{i=1}^{N}\beta_i\xi_i }$</li><li>限制条件 (Subject to) :  $i=1,2,\dots,N$<ul><li>$\alpha_i\geq0$ or $\alpha\succeq0$</li><li>$\beta_i\geq0$ or $\beta\succeq0$</li></ul></li></ul><p><strong>解释：</strong>其中优化目标中的 $\alpha$ 和 $\beta$ 与 <strong>附录——优化理论：原问题与对偶问题</strong>中的拉格朗日乘子中稍微有些不同，在原拉格朗日乘子中 $\alpha$ 控制限制条件中不等式， $\beta$ 控制限制条件中的等式。由于我们转换后的非线性SVM优化问题的限制条件中不存在等式 $h<em>i(w)=0$ 这一项，故而非线性SVM优化问题对应的拉格朗日乘子 $L(w,\alpha,\beta)=f(w)+\sum \limits</em>{i=1}^{K}\alpha<em>ig_i(w)+\sum \limits</em>{i=1}^{M}\beta<em>ih_i(w)$ 中不存在 $\sum \limits</em>{i=1}^{M}\beta_ih_i(w)$ 项。而我们转换后的非线性SVM优化问题的限制条件中存在两个不等式，故而非线性SVM优化问题对应的拉格朗日乘子包含了两个控制不等式的 $\alpha$ ,这里是为了在同一个式子中便于区分故而分别写为了 $\alpha,\beta$, 我们转化后的对偶问题中的 $\alpha,\beta$ 都对应拉格朗日乘子中的 $\alpha$.  对于限制条件中 $\alpha_i\geq0$ 与 $\alpha\succeq$0 的写法不同但含义相同，只不过前者表示向量 $\alpha$ 中的每一项都大于0，而后者表示向量 $\alpha$ 大于0.  </p></li><li><p>[ ] <strong>Step 3: </strong>求对偶问题的解，即要求出一组 $w,\xi_i,b$ 使优化目标中 ${\cdots}$ 部分最小。</p><p>利用求导法，令</p><script type="math/tex; mode=display">\begin{align}&\frac{\partial L}{\partial\omega}=0 \ \ \ \Longrightarrow w-\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i)=0 \ \ \ \Longrightarrow w=\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i) \\&\frac{\partial L}{\partial\xi_i}=0 \ \ \ \Longrightarrow \alpha_i+\beta_i=C \ \ \ \Longrightarrow \alpha_i+\beta_i=C \\&\frac{\partial L}{\partial b}=0 \ \ \ \Longrightarrow \sum_{i=1}^{N}\alpha_iy_i=0\end{align}</script><p>将上式代入 $\theta(\alpha,\beta)$ 得：</p><script type="math/tex; mode=display">\begin{align}\theta(\alpha,\beta)&=\inf \limits_{w,\xi_i,b}\{\frac{1}{2}||w||^2-C \sum\limits_{i=1}^N\xi_i+\sum_{i=1}^{N}\alpha_i[1+\xi_i-y_iw^T\varphi(x_i)-y_ib]+\sum_{i=1}^{N}\beta_i\xi_i \} \\&=\inf \limits_{w,\xi_i,b} \{\frac{1}{2}w^Tw-\bcancel{\sum_{i=1}^{N}\alpha_i\xi_i}-\bcancel{\sum_{i=1}^{N}\beta_i\xi_i}+\sum_{i=1}^{N}\alpha_i+\bcancel{\sum_{i=1}^{N}\alpha_i\xi_i}-\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i)w^T-\bcancel{\sum_{i=1}^{N}\alpha_iy_ib}+\bcancel{\sum_{i=1}^{N}\beta_i\xi_i} \} \\&=\inf \limits_{w,\xi_i,b} \{\frac{1}{2}\big(\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i)\big)^T\big(\sum_{j=1}^{N}\alpha_jy_j\varphi(x_j)\big)+\sum_{i=1}^{N}\alpha_i-\big(\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i)\big)^T\big(\sum_{j=1}^{N}\alpha_jy_j\varphi(x_j)\big) \} \\&=\inf \limits_{w,\xi_i,b} \{\sum_{i=1}^{N}\alpha_i-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\underbrace{\alpha_i\alpha_j}_{常数}\underbrace{y_iy_j}_{y=\pm1}\underbrace{\varphi(x_i)^T\varphi(x_j)}_{K(x_i,x_j)}\}\end{align}</script><p>最后，对偶问题可以转换为：</p><h6 id="非线性SVM优化问题的对偶问题-求解后"><a href="#非线性SVM优化问题的对偶问题-求解后" class="headerlink" title="非线性SVM优化问题的对偶问题 (求解后)"></a>非线性SVM优化问题的对偶问题 (求解后)</h6><ul><li>最大化 (Maximum) :         $\theta(\alpha)=\sum\limits<em>{i=1}^{N}\alpha_i-\frac{1}{2}\sum\limits</em>{i=1}^{N}\sum\limits<em>{j=1}^{N}\alpha_i\alpha_j\underbrace{y_jy_i}</em>{已知}\underbrace{K(x<em>i,x_j)}</em>{已知}$</li><li>限制条件 (Subject to) :  $i=1,2,\dots,N$<ul><li>$\alpha\geq0, \&amp;\&amp;\  \beta_i\geq0 \ \ \ \Longrightarrow\ 0\leq\alpha_i\leq C$</li><li>$\sum\limits_{i=1}^{N}\alpha_iy_i=0$</li></ul></li></ul><p>此时，该问题变为了一个基本的凸优化问题，我们可以使用 <a href="https://en.wikipedia.org/wiki/Sequential_minimal_optimization"><strong>SMO算法</strong></a> 对其进行求解。</p></li><li><p>[ ] <strong>Step 4: </strong>将求 $\alpha$ 转化为求 $w,b$.</p><p>由 <strong>Step 3</strong> 有 $w=\sum\limits_{i=1}^{N}\alpha_iy_i\varphi(x_i)$ ，但其中仍然存在高维向量 $\varphi(x_i)$ 似乎无法求解，但实际上，我们对于一个测试样本 $x$ ,则有:</p><script type="math/tex; mode=display">\left\{ \begin{array}{rcl} 若w^T\varphi(x)+b\geq0,则\ y=+1\\若w^T\varphi(x)+b<0,则\ y=-1\end{array}\right.</script><p>其中 $w^T\varphi(x)=\sum\limits<em>{i=1}^{N}\alpha_iy_i\varphi(x_i)^T\varphi(x_i)\ \  \Longrightarrow \sum\limits</em>{i=1}^{N}\alpha_iy_iK(x_i,x)$ , 即我们不需要知道 $w$ 具体是多少，我们可以直接算出 $w^T\varphi(x)$ 的值。</p><p>现在只需要求出 $b$ 即可，利用 <strong>KKT条件</strong> 求 $b$. </p><p><strong>KKT条件：</strong>对于  $\forall i=1,2,\dots,K.$ 则有 $\alpha_i^<em>=0$ 或者 $g_i(w^</em>)=0$ .<br><strong>转化为SVM中的KKT</strong>，则有 $\forall\  i=1,2,\dots,N$</p><pre><code>                                     $\blacktriangleright$ 要么 $ \beta_i=0$,  要么 $\xi_i=0$                                     $\blacktriangleright$ 要么 $\alpha_i=0$, 要么 $1+\xi_i-y_iw^T\varphi(x_i)-y_ib=0$</code></pre><p>因此，我们随机选取一个 $\alpha_i$ , 则 $0&lt;\alpha_i<C$， 则 $\beta_i=C-\alpha_i>0$</p><p>​                                        $\therefore \beta_i\neq0, \ 则\ \xi_i=0$,</p><p>​                                        $\therefore \alpha_i\neq0, \ 则 \ 1+\xi_i-y_iw^T\varphi(x_i)-y_ib=0$</p><p>​                                        $\therefore b=\frac{1}{y<em>i}-\sum</em>{i=1}^{N}\alpha_iy_iK(x_i,x)$</p><p>至此，非线性SVM的优化问题求解完毕！</p></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h4 id="SVM算法：训练过程"><a href="#SVM算法：训练过程" class="headerlink" title="SVM算法：训练过程"></a>SVM算法：训练过程</h4><p>输入 ${(x<em>i,y_i)}</em>{i=1,2,\dots,N}$，解优化问题</p><ul><li>最大化 (Maximum) :         $\theta(\alpha)=\sum\limits<em>{i=1}^{N}\alpha_i-\frac{1}{2}\sum\limits</em>{i=1}^{N}\sum\limits<em>{j=1}^{N}\alpha_i\alpha_j\underbrace{y_jy_i}</em>{已知}\underbrace{K(x<em>i,x_j)}</em>{已知}$</li><li>限制条件 (Subject to) :  $i=1,2,\dots,N$<ul><li>$\alpha\geq0, \&amp;\&amp;\  \beta_i\geq0 \ \ \ \Longrightarrow\ 0\leq\alpha_i\leq C$</li><li>$\sum\limits_{i=1}^{N}\alpha_iy_i=0$</li></ul></li></ul><p>利用SMO算法求解最优化问题，得出 $\alpha$, 下一步寻找一个 $0&lt;\alpha_i&lt;0$，计算 $b$ :</p><p>​          $b=\frac{1}{y<em>i}-\sum</em>{i=1}^{N}\alpha_iy_iK(x_i,x)$</p><h4 id="SVM算法：测试过程"><a href="#SVM算法：测试过程" class="headerlink" title="SVM算法：测试过程"></a>SVM算法：测试过程</h4><p>输入样本 $x$：</p><ul><li>若 $\sum\limits_{i=1}^{N}\alpha_iy_iK(x_i,x)+b\geq0$, 则 $ y=+1$,</li><li>若 $\sum\limits_{i=1}^{N}\alpha_iy_iK(x_i,x)+b&lt;0$, 则 $ y=+1$.</li></ul><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h4 id="结论1证明："><a href="#结论1证明：" class="headerlink" title="结论1证明："></a>结论1证明：</h4><h5 id="二维平面上的证明"><a href="#二维平面上的证明" class="headerlink" title="二维平面上的证明"></a>二维平面上的证明</h5><p>假设我们有两个点集 $A$ 和 $B$，并且这两个点集是线性可分的。这意味着存在一条直线可以将这两个点集分开。为了证明存在无数条直线能够划分这两个点集，我们可以如下进行证明：</p><ol><li><p><strong>存在一条直线</strong>：假设存在一条直线 $L$ 可以将点集 $A$ 和 $B$ 分开。直线 $L$ 可以表示为：</p><script type="math/tex; mode=display">𝑦=k𝑥+b</script><p>其中 $k$ 是斜率，$b$ 是截距。</p></li><li><p><strong>平行直线的性质</strong>：对于任何一个固定的斜率 𝑚<em>m</em>，不同的截距 𝑐<em>c</em> 会产生不同的平行直线。设我们有另一条直线 𝐿′<em>L</em>′ 其方程为：</p><script type="math/tex; mode=display">𝑦=𝑚𝑥+b^′</script><p>其中 $b’\neq b$，则 $L’$ 与 $L$ 平行。</p></li><li><p><strong>平行直线的分离能力</strong>：由于 $A$ 和 $B$ 是线性可分的，这意味着可以找到一个间隔  $\epsilon$ 使得在直线  $L$的一侧存在一个空隙，其中没有任何点在此区域。换句话说，我们可以调整 $b$ 的值，使得新的直线 $L’$ 依然可以将 $A$ 和 $B$ 分开。</p></li><li><p><strong>无数条平行直线</strong>：由于 $b$ 可以在实数范围内任意取值，因此存在无数个不同的 $b$ 值，这意味着存在无数条平行直线可以将 $A$ 和 $B$ 分开。</p></li><li><p><strong>非平行直线</strong>：除了平行直线之外，我们还可以选择不同的斜率 $k’$。对任何新的斜率 $k’$，只要新的直线方程能够满足分离点集的条件，我们就可以调整其截距 $b’$ 使得新的直线 $y=k’x+b’$ 依然可以将 $A$ 和 $B$ 分开。</p></li></ol><h5 id="高维空间的推广"><a href="#高维空间的推广" class="headerlink" title="高维空间的推广"></a>高维空间的推广</h5><p>在高维空间中，能够分离点集的“直线”实际上是一个超平面。假设我们有两个点集 $A$ 和 $B$，并且它们是线性可分的，即存在一个超平面 $H$ 可以将它们分开。超平面方程可以表示为：</p><script type="math/tex; mode=display">w\cdot x+b=0</script><p>其中 $w$ 是法向量，$b$ 是偏置项。</p><ol><li><strong>存在一个超平面</strong>：假设存在一个超平面 $H$ 可以将 $A$ 和 $B$ 分开。</li><li><strong>平行超平面</strong>：对于任何固定的法向量 $w$，不同的偏置 $b$ 会产生不同的平行超平面。</li><li><strong>无数个平行超平面</strong>：由于 $b$ 可以在实数范围内任意取值，因此存在无数个不同的 $b$ 值，这意味着存在无数个平行超平面可以将 𝐴<em>A</em> 和 𝐵<em>B</em> 分开。</li><li><strong>非平行超平面</strong>：我们还可以选择不同的法向量 $w’$。对于任何新的法向量 $w’$，只要新的超平面方程能够满足分离点集的条件，我们就可以调整其偏置 $b$ 使得新的超平面 $w’\cdot x+b’=0$ 依然可以将 $A$ 和 $B$ 分开。</li></ol><h4 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h4><ul><li><p>在实际应用时，映射函数 $\varphi(x)$ 不需要是已知的。换句话说，核技巧的目的就是在不需要显式地定义特征空间和映射函数的条件下，计算映射之后的内积</p></li><li><p>核函数 $K(x_1,x_2)=&lt;\varphi(x_1),\varphi(x_2)&gt;$ 需要满足的充要条件 (不需要 $\varphi(x)$ 已知) (<a href="[Mercer&#39;s theorem - Wikipedia](https://en.wikipedia.org/wiki/Mercer&#39;s_theorem">Mercer’s Theorem</a>)): </p><ul><li><p>对称性: $K(x_1,x_2)=K(x_2,x_1)$</p></li><li><p>半正定性: 对于任意 $n$ 和任意 $x_1,x_2,\dots,x_n∈\mathcal{X}$，由 $K(x_i,x_j)$ 定义的 Gram matrix 总是半正定的，即</p><script type="math/tex; mode=display">对于\forall C_i,x_i, (i=1，2,\dots,n),有 \sum_{i=1}^N\sum_{j=1}^NC_iC_jK(x_1,x_2)\geq0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ C_i为任意实数，x_i为任意的向量.</script></li></ul></li><li><p>只要 $K$ 是核函数，那么一定存在一个Hilbert space和一个映射函数 $\varphi(x)$，使得 $K(x_1,x_2)=&lt;\varphi(x_1),\varphi(x_2)&gt;$</p></li><li><p>常见核函数</p><p>| 名称       | 表达式                                              | 参数                                           |<br>| ————— | —————————————————————————- | ——————————————————————— |<br>| 线性核     | $K(x_1,x_2)=x_i^Tx_j$                               |                                                |<br>| 多项式核   | $K(x_1,x_2)=(x_i^Tx_j)^d$                           | $d\geq1$为多项式的次数                         |<br>| 高斯核     | $K(x_1,x_2)=\exp(-\frac{||x_i-x_j||^2}{2\sigma^2})$ | $\sigma&gt;0$，为高斯核的带宽 (width)             |<br>| 拉普拉斯核 | $K(x_1,x_2)=\exp(-\frac{||x_i-x_j||}{\sigma})$      | $\sigma&gt;0$                                     |<br>| Sigmoid核  | $K(x_1,x_2)=\tanh(\beta x_i^Tx_j+\theta)$           | $\tanh$ 为双曲正切函数，$\beta&gt;0,\ \ \theta&lt;0$ |</p><h4 id="替代损失函数"><a href="#替代损失函数" class="headerlink" title="替代损失函数"></a>替代损失函数</h4><p>| 名称                            | 表达式                                |<br>| ———————————————- | ——————————————————- |<br>| hinge损失函数                   | $\mathscr{l}<em>{hinge}(z)=\max(0,1-z)$  |<br>| 指数损失函数 (exponential loss) | $\mathscr{l}</em>{exp}(z)=\exp(-z)$       |<br>| 对率损失 (logistic loss)        | $\mathscr{l}_log(z)=\log(1+\exp(-z))$ |</p><p><img src="https://imgur.la/images/2024/06/04/surrogate_loss.png" alt="surrogate loss" border="0" style="zoom: 5%;" ></p><center><p>图5：三种常见的替代损失函数: hinge损失、指数损失、对率损失</p></center></li></ul><h4 id="优化理论：原问题与对偶问题"><a href="#优化理论：原问题与对偶问题" class="headerlink" title="优化理论：原问题与对偶问题"></a>优化理论：原问题与对偶问题</h4><p><strong>1. 原问题 (Prime Problem)</strong></p><ul><li>最小化 (Minimize) : $f(w)$</li><li>限制条件 (Subject to) : <ul><li>$g_i(w)\leq0 \ \ \ \ (i=1,2,\dots,K)$</li><li>$h_i(w)=0 \ \ \ \ (i=1,2,\dots,M)$</li></ul></li></ul><p>对原问题使用拉格朗日乘子法可以得到其”对偶问题”</p><script type="math/tex; mode=display">\begin{align}&L(w,\alpha,\beta) \\=&f(w)+\sum \limits_{i=1}^{K}\alpha_ig_i(w)+\sum \limits_{i=1}^{M}\beta_ih_i(w) \\=&f(w)+\alpha^Tg(w)+\beta^Th(w)\end{align}</script><p>根据您上述过程，我们可以得到原问题的对偶问题：</p><p><strong>2. 对偶问题 (Dual Problem)</strong></p><ul><li>最大化 (Maximum) : $\theta(\alpha,\beta)=\inf{L(w,\alpha,\beta)}$</li><li>限制条件 (Subject to) : <ul><li>$\alpha_i\geq0 \ \ \ \ (i=1,2,\dots,K)$</li></ul></li></ul><p><strong>解释：</strong></p><p>其中 $\inf{L(w,\alpha,\beta)}$ 表示在限定的 $\alpha,\beta$ 的条件下，遍历所有的 $w$，求 $L(w,\alpha,\beta)$ 的最小值。因此对于每确定的一组 $\alpha,\beta$ 都可以算出 $L(w,\alpha,\beta)$ 的最小值。而我们此时要找的就是所有的能算出 $L(w,\alpha,\beta)$ 最小值的 $\alpha,\beta$ 中最大的那一组 $\alpha,\beta$。</p><p><strong>3. 将原问题的解转化为对偶问题的解</strong></p><p><strong>定理：</strong>如果 $w^<em>$是原问题的解，而$\alpha^</em>,\beta^<em>$是对偶问题的解，则有$f(w^</em>)\geq\theta(\alpha^<em>,\beta^</em>)$</p><p>证明：(其中 $\alpha^<em>\geq0, \ \ g_i(w^</em>)\leq0, \ \ h_i(w)=0$)</p><script type="math/tex; mode=display">\begin{align}\theta(\alpha^*,\beta^*)&=\inf\{L(w,\alpha^*,\beta^*)\} \\&\leq L(w^*,\alpha^*,\beta^*) \\&=f(w^*)+\sum \limits_{i=1}^{K}\alpha_i^*g_i(w^*)+\sum \limits_{i=1}^{M}\beta_i^*h_i(w^*) \\&\leq f(w^*)\end{align}</script><p><strong>定义：</strong>$G=f(\omega^<em>)-\theta(\alpha^</em>,\beta^*)\geq0$,  $G$ 为原问题与对偶问题的间距 (Duality Gap)。</p><p><strong>强对偶定理：</strong>若 $f(w)$ 为凸函数，且 $g(w)=aw+b, \ \ h(w)=cw+d$ (即其约束条件均为线性函数), 则此优化问题得原问题与对偶问题的对偶间距 $G=0$.</p><p>由强对偶性定理得：</p><script type="math/tex; mode=display">f(w^*)=\theta(\alpha^*,\beta^*)</script><p>此时意味着，对于  $\forall i=1,2,\dots,K.$ 则有 $\alpha_i^<em>=0$ 或者 $g_i(w^</em>)=0$ .  <strong>——KKT条件</strong>  </p><hr><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><blockquote><p>[1] 周志华，机器学习，清华大学出版社，2016</p><p>[2] Boyd S P, Vandenberghe L. Convex optimization[M]. Cambridge university press, 2004.</p><p>[3] Cortes C, Vapnik V. Support-vector networks[J]. Machine learning, 1995, 20: 273-297.</p><p>[4] Ruszczyński A P. Nonlinear optimization[M]. Princeton university press, 2006.</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Supervised machine learning </tag>
            
            <tag> Convex optimization </tag>
            
            <tag> Dural Problem </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>markov_process</title>
      <link href="/2023/12/27/markov-process/"/>
      <url>/2023/12/27/markov-process/</url>
      
        <content type="html"><![CDATA[<h2 id="马尔可夫过程-State-Probability"><a href="#马尔可夫过程-State-Probability" class="headerlink" title="马尔可夫过程 (State + Probability)"></a>马尔可夫过程 (State + Probability)</h2><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Markov_process-example.svg/2880px-Markov_process-example.svg.png" alt="undefined" style="zoom:50%;" /></p><h3 id="状态（State）"><a href="#状态（State）" class="headerlink" title="状态（State）"></a>状态（State）</h3><p><strong>定义：</strong>具有<strong>马尔可夫性质</strong>的有限随机状态序列 <strong>$S_1, S_2, … …$</strong></p><p>马尔可夫性质（markov property）：当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；</p><p> $P(S<em>{t+1} | S_t) = P(S</em>{t+1}|S_1, …, S_t)$   <strong>无后效性</strong></p><p>解释：从$S_1, …, S_t$所蕴含的信息与$S_t$是等价的， 当前的状态只由上一个状态所决定，而不受更早之前状态影响。</p><h3 id="转移概率（Transition-Probability）"><a href="#转移概率（Transition-Probability）" class="headerlink" title="转移概率（Transition Probability）"></a>转移概率（Transition Probability）</h3><p>状态转移矩阵</p><p><img src="C:\Users\S\AppData\Roaming\Typora\typora-user-images\image-20231228110315549.png" alt="image-20231228110315549" style="zoom:50%;" /><img src="C:\Users\S\AppData\Roaming\Typora\typora-user-images\image-20231228110454720.png" alt="image-20231228110454720" style="zoom:50%;" /></p><h2 id="马尔可夫奖励过程（State-Probability-Reward）"><a href="#马尔可夫奖励过程（State-Probability-Reward）" class="headerlink" title="马尔可夫奖励过程（State + Probability+Reward）"></a>马尔可夫奖励过程（State + Probability+Reward）</h2><p>马尔可夫过程+奖励=马尔可夫奖励过程  </p><p>奖励：</p><p>即时奖励：$R_{t+1}$</p><p>其中即时奖励与状态转移相伴随： $R<em>{t+1}=f(S_t \rightarrow S</em>{t+1})$</p><p>只要发生状态转移就会产生即时奖励。</p><p>长期奖励：$G_t$</p><p>$G<em>t = R</em>{t+1}+\gamma R<em>{t+2}+\gamma ^ 2R</em>{t+3}+\dots=\sum \limits<em>{k=0}^{\infty}\gamma^k R</em>{t+k+1}$ </p><p>$\gamma:$  衰减系数，表示即时奖励在当前的折扣值， $\gamma \in [0,1]$</p><p>$\gamma$   越接近0， 更喜欢现在的reward；$\gamma$  越接近1， 越不关心现在已经得到的reward。</p><p>长期奖励与状态转移链相伴随， 不同的状态转移链对应不同的长期奖励。             </p><p>价值函数：$V(s)$</p><p>评价状态 $S$ 的质量，使用以当前状态 $S$ 起点的所有状态转移链的 $G_t$ 的期望来作为衡量 $S$ 的价值指标</p><script type="math/tex; mode=display">\begin{align}V(S)&=E[G_t | S_t=S]  \\&=E[R_{t+1}+\gamma R_{t+2}+\gamma ^ 2R_{t+3}+\dots | S_t = S] \\&=E[R_{t+1}+(\gamma R_{t+2}+\gamma ^ 2R_{t+3}+\dots) | S_t = S] \\&=E[R_{t+1}+\gamma G_{t+1} | S_t = S] \\&=E[R_{t+1}+\gamma V(S_t+1) | S_t = S]\end{align}</script><p>$V(S)$  取决于前一个状态的奖励 $R_{t+1}$，也取决于当前状态的</p><p>Bellman Equation</p><h3 id="马尔可夫决策过程-（State-Probability-Reward-Action）"><a href="#马尔可夫决策过程-（State-Probability-Reward-Action）" class="headerlink" title="马尔可夫决策过程 （State + Probability + Reward + Action）"></a>马尔可夫决策过程 （State + Probability + Reward + Action）</h3><p>MDP 元组定义 $\langle S,A,P,R,\gamma\rangle$：</p><p>$S$  有限状态集</p><p>$A$  有限动作集</p><p>$P$  状态转移概率矩阵， $P<em>{ss’}^a=P[S</em>{t+1}=s’ | S_t=s, A_t=a]$</p><p>$R$  奖励函数， $R<em>s^a=E[R</em>{t+1}|S_t=s, A_t=a]$</p><p>$\gamma$  折扣函数， $\gamma\in[0, 1]$</p><p>目标：找到一条最佳路径，使得这条路径上的Reward最大</p><p>给定状态的动作概率分布（policy）：$\pi$</p><script type="math/tex; mode=display">\pi(a | s)=P[A_t=a|S_t=s]</script><p>状态价值函数  $v_{\pi}(s)$ </p><script type="math/tex; mode=display">\begin{align}v_\pi(s) &= E_\pi[G_t | S_t=s] \\&=E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1}) | S_t=s]\end{align}</script><p>从状态  $s$  开始，遵循policy  $\pi$，期望获得的累计奖励。</p><p>动作价值函数  $q_\pi(s,a)$</p><script type="math/tex; mode=display">\begin{align}q_\pi(s,a)&=E_\pi [G_t|S_t=s, A_t=a] \\&=E_\pi [R_{t+1}+\gamma q_\pi(S_{t+1}, A_{t+1})|S_t=s, A_t=a] \\&=R_s^a+\gamma \sum \limits_{s'\in S}P_{ss'}^av_\pi(s')\end{align}</script><p>从状态  $s$  开始，执行动作  $a$,  遵循policy $\pi$  期望获得的累计奖励。</p><p>包含动作的状态转移过程：</p><p><img src="C:\Users\S\AppData\Roaming\Typora\typora-user-images\image-20231229150527704.png" alt="image-20231229150527704" style="zoom:50%;" /><img src="C:\Users\S\AppData\Roaming\Typora\typora-user-images\image-20231229150542970.png" alt="image-20231229150542970" style="zoom:50%;" /><img src="C:\Users\S\AppData\Roaming\Typora\typora-user-images\image-20231229150558157.png" alt="image-20231229150558157" style="zoom:50%;" /><img src="C:\Users\S\AppData\Roaming\Typora\typora-user-images\image-20231229160323899.png" alt="image-20231229160323899" style="zoom:50%;" /></p><p>贝尔曼期望方程求解 $V^\pi$</p><script type="math/tex; mode=display">v_\pi(S)=\sum \limits_{a\in A}\pi(a|s)q_\pi(s,a)</script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/12/08/hello-world/"/>
      <url>/2023/12/08/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
